{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Model building. \"Recommender\" case study: Restaurants"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Goals / Topics\n",
      " - Types of recommenders\n",
      " - Simple examples\n",
      " - \"Page rank\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##The problem: Recommendations\n",
      "\n",
      "For a Yelp user, suggest another ."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "**The type of learner**: TODO.\n",
      "\n",
      "**The training dataset**: We'll use [Yelp's academic data set](https://www.yelp.com/academic_dataset) -- it contains a sampling Yelp user, business, and review data from around some universities.\n",
      "\n",
      "**The test dataset**: We.. won't do it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip, json, itertools, collections\n",
      "import networkx as nx\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Types of recommenders\n",
      "There are two primary high-level types of recommender systems:\n",
      "\n",
      " 1. **Content-based filtering**: Content-based recommender systems examine properties of the items being recommended.  For instance, if a user likes Italian restaurants in Austin, TX with a strong wine list -- try to recommend more of this. \n",
      " 2. **Collaborative filtering**: Collaborative filtering recommender systems examine similarity between users and/or items, so that the items recommended to a user are those liked by similar users. \n",
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rows = map(json.loads, gzip.open(\"yelp-project/data/raw/yelp_academic_dataset.json.gz\").readlines())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collections.Counter(r['type'] for r in rows)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_df = pd.DataFrame([r for r in rows if r['type'] == 'user'])\n",
      "review_df = pd.DataFrame([r for r in rows if r['type'] == 'review'])\n",
      "business_df = pd.DataFrame([r for r in rows if r['type'] == 'business'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "business_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "review_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_df['review_count'].apply(lambda x: np.log(0.0001+x)).hist(bins=50)\n",
      "plt.title(\"Histogram of Log Review Count, by User\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "business_df['review_count'].apply(lambda x: np.log(0.0001+x)).hist(bins=50)\n",
      "plt.title(\"Histogram of Log Review Count, by Business\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Content-based filtering: Regression\n",
      "\n",
      "We can try to build a _supervised regression_ to try to predict how much a user will like a given (unseen) business.  For each user -- let's call our test user Alice -- we will build a model \n",
      "$$ \\text{Review} \\mapsto s_A(\\text{Review}) $$\n",
      "\n",
      "To make a recommendation / recommendations, we will run the model on a set of potential un-visited businesses -- and then we will recommend the business with highest score(s).\n",
      "\n",
      "Some potential features might be:\n",
      " - The tf-idf vector of the business' reviews\n",
      " - The category of business / of restaurant / etc.\n",
      " - The average review score for the business\n",
      " - Location (in some form)\n",
      " - The business' similarity to those that the uesr has rated at 5-stars (say in terms of cosine similarity)\n",
      " - ...\n",
      " \n",
      "Meanwhile, the regressed quantity will be the star score of the review.  (Recall the discussion of normalizing it to be between 0 and 1, taking the logit, etc.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Variation:\n",
      "We could instead have trained a _classifier_ to predict if a user will like (e.g., 4 or 5 start review) a given business."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Content-based filtering: Similarity to a user profile vector\n",
      "A variant of the above is as follows: For each business, generate a feature vector roughly as above.  Then, generate a \"user profile\" vector for each user -- e.g., by taking the mean of the feature vectors weighted by positive / negative scores based on the user's review of the business.\n",
      "\n",
      "Now apply e.g., cosine similarity to see how similar a given  business is to the user's preferences (as determined by the user profile vector)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Exercise:\n",
      "Try implementing the second of these (i.e., a user profile vector) with the example features above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Collaborative filtering: Identifying similar users / business\n",
      "\n",
      "One possible approach is the follows:\n",
      " - Take a user's vector of all *de-meaned* review scores: That is, if they did not give a review the value is zero; if they did, it is the score _minus_ the user's average review score.  This way a blank review counts as \"nothing\", while a positive / negative review contributes a positive / negative number to the vector.\n",
      " - This gives rise to a \n",
      " $ \\# \\text{users} \\times \\# \\text{businesses} $\n",
      " matrix of scores.  This is the (normalized) __utility matrix__.\n",
      " - We can now use cosine similarity to identify simiilar users.\n",
      " \n",
      "### Variant: Binary (have / have not reviewed) and Jaccard distance.\n",
      "\n",
      "There is a dual picture for identifying similar businesses: We will again construct a \n",
      "$ \\# \\text{users} \\times \\# \\text{businesses} $\n",
      "matrix of normalized scores, only now we subtract out the average review of the business (rather than from the user).\n",
      "\n",
      "In practice, the matrix of reviews is likely to be quite sparse -- so that this may not work well.  There are several possible solutions to this:\n",
      " - cluster users / businesses first;\n",
      " - try to dimensionally reduce via \"matrix factorization\".  That is, __factor__ the utility matrix as a product of two long-and-thin matrices."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Exercise:\n",
      "Build a simple \"you may also like _this_ business\" recommender by using the method suggested above: i.e., take the utility matrix adjusted so that the non-zero entries for each business have been shifted to have mean zero, and then apply cosine similarity.\n",
      "\n",
      "The following block has a starting point (to populate the columns of the adjusted utility matrix)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def adjusted_rev_business(business_id):\n",
      "    reviews = review_df[ review_df.business_id == business_id ]\n",
      "    reviews['stars'] = reviews['stars'] - reviews['stars'].mean()\n",
      "    return user_df.merge( reviews, how='left', on='user_id')['stars'].fillna(0).values\n",
      "    \n",
      "ret = adjusted_rev_business('piXuRfZ81xFGA64WFJrKkQ')\n",
      "\n",
      "print ret\n",
      "#for i in range(len(ret)):\n",
      "#    if ret[i]!=0:\n",
      "#        print i, ret[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fill this in:\n",
      "# You do not want to just use adjusted_rev_business above: it'll be too slow because it goes through all reviews for every business, while we should be able to just read go through the reviews once.\n",
      "\n",
      "#adjusted_matrix = ..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import heapq\n",
      "\n",
      "def cos_sim(v, w):\n",
      "    return v.dot(w)/np.sqrt(v.dot(v) * w.dot(w))\n",
      "\n",
      "def find_similar_businesses(n, business_id):\n",
      "    to_find = adjusted_rev_business(business_id)\n",
      "    return map(lambda x: x[0], \n",
      "               heapq.nlargest(n, adjusted_matrix, \n",
      "                              key=lambda x: cos_sim(to_find, x[1])\n",
      "                                ))\n",
      "\n",
      "# Uncomment this after filling in the above:\n",
      "# print find_similar_businesses(10, 'piXuRfZ81xFGA64WFJrKkQ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Graph based collaborative filtering\n",
      "We're looking for an excuse to talk about PageRank (and similar things). To that end, we will think of users, businesses, and graphs as a __bipartite graph__.  We will try to make recommendations based on the properties of this graph.\n",
      "\n",
      "We will talk about two types of algorithms:\n",
      "1. __HITS__: Also known as _hubs and authorities_.  This was a precursor to PageRank.  It's based on splitting a graph into \"hubs\" and \"authorities\" and implementing the idea that authorities (resp., hubs) are important if they are linked to (resp., from) important authorities (resp., hubs).  For a description see http://en.wikipedia.org/wiki/HITS_algorithm#Algorithm\n",
      "\n",
      "2. __PageRank__: PageRank gets rid of the bi-partite nature of HITS.  The idea now is that a node is important if important nodes link to it, with no split.  That sounds great, but has obvious problems (especially in a directed graph): imagine that there is a node that receive links, but has no outgoing links, it will necessarily seem very very important.  To address this, there is an added \"teleportation\" factor.  For a detailed description see http://en.wikipedia.org/wiki/PageRank#Algorithm (or Google around for lecture notes).  Here's the basic idea:\n",
      "\n",
      "  Begin by placing a billion people randomly on the nodes of the graph.  At each time step, each person undergoes the following operation\n",
      "  - With probability p% (small), the person teleports to a randomly chosen other node.\n",
      "  - With probability (1-p)% (big), the person goes to a randomly selected neighbor of the node that they are at now.\n",
      "  \n",
      "  Then, the PageRank is the limit distribution of where the people end up.  In other words, it is the eigenvector (with maximal eigenvalue) of a certain _modified adjacency matrix_:\n",
      "  \n",
      "  $$ \\frac{p}{N} \\, \\mathrm{id} + (1-p) \\, A \\, \\mathrm{diag}(D)^{-1} $$\n",
      "  \n",
      "  with $N$ the number of nodes, $A$ the adjacency matrix, and $\\mathrm{diag}(D)$ the diagonal matrix of edge degrees.\n",
      "\n",
      "3. __(Personalized) PageRank__: This is a variant of PageRank that is suitable for e.g. individualized recommendations.  Here, the random jumping is restricted to a specific node.\n",
      "\n",
      "\n",
      "Below, we give simple example of using HITS and Personalized PageRank for recommendations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build the directed graph of reviews.  For simplicity we'll restrict to popular (and positive) ones.\n",
      "biz = set( business_df[business_df.review_count > 60]['business_id'] )\n",
      "usr = set( user_df[user_df.review_count > 40]['user_id'] )\n",
      "revs = review_df[ review_df['user_id'].apply(lambda x: x in usr) & review_df['business_id'].apply(lambda x: x in biz)  ]\n",
      "\n",
      "G = nx.Graph()\n",
      "\n",
      "to_usr = lambda x: ('u', x)\n",
      "to_biz = lambda x: ('b', x)\n",
      "G.add_nodes_from(map(to_biz, biz))\n",
      "G.add_nodes_from(map(to_usr, usr))\n",
      "#G.add_edges_from( zip(revs['user_id'], revs['business_id'] ))\n",
      "G.add_weighted_edges_from( zip(\n",
      "                        map(to_usr, revs['user_id']), \n",
      "                        map(to_biz, revs['business_id']),\n",
      "                        revs['stars'])  )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### HITS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hubs, authorities = nx.hits_scipy(G, max_iter=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lookup_ub(x):\n",
      "    (t, xid) = x\n",
      "    if t=='u':\n",
      "        return user_df[ user_df['user_id']==xid ].values\n",
      "    else:\n",
      "        return business_df[ business_df['business_id']==xid ].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[lookup_ub(k) for k,v in authorities.iteritems() if v>0.5*1e-3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[lookup_ub(k) for k,v in hubs.iteritems() if v>1e-3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Personalized PageRank"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_user = \"zxtZqGUAlyi5MvsxKrDzVQ\"\n",
      "print lookup_ub(('u', target_user))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "personalization = { k : 0 for k in G.nodes() }\n",
      "personalization[to_usr(target_user)] = 1.0\n",
      "pr = nx.pagerank_scipy(G, personalization = personalization, max_iter=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[lookup_ub(k) for k,v in pr.iteritems() if v>1e-2 and k[0]=='b']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[lookup_ub(k) for k,v in pr.iteritems() if v>1e-3 and k[0]=='u']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### A little bit of visualization\n",
      "The main observation we might make is that these recommendations all seem to be around Austin.  Let's perform a simple visualization exercise: \n",
      "\n",
      ">          \n",
      "    For a given user, we will map their recommendations.\n",
      "\n",
      "We will make this a little bit interactive, by allowing the user to flip amongst a randomly selected (small) sample of users for whom to do this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import folium\n",
      "import IPython.display as display\n",
      "from unidecode import unidecode\n",
      "\n",
      "def show_map(m):\n",
      "    m._build_map()\n",
      "    inner_html = m.HTML.replace('\"', '&quot;')  ## This is a it of a hack!\n",
      "    inner_html = inner_html.replace(\"http://cdn.leaflet\", \"https://cdn.leaflet\")\n",
      "    return display.display_html('<iframe srcdoc=\"{0}\" style=\"width: 100%; height: 510px; border: none\"></iframe>'.format(inner_html), \n",
      "                         raw=True)\n",
      "\n",
      "def map_businesses(items_df): \n",
      "    if items_df.empty:\n",
      "        return\n",
      "    m = folium.Map(location=[items_df.latitude.values[0], items_df.longitude.values[0]], zoom_start=13)\n",
      "    for lat, lon, name in items_df[['latitude', 'longitude', 'name']].values:\n",
      "        m.circle_marker([lat, lon], popup = unidecode(name), radius=50, fill_color=\"red\", line_color=\"black\", fill_opacity=0.9)\n",
      "    return show_map(m)\n",
      "\n",
      "def map_biz_by_id(ids):\n",
      "    items_df = pd.concat( [ business_df[ business_df.business_id == bid ] for bid in ids] )\n",
      "    return map_businesses(items_df)\n",
      "\n",
      "def map_user_recs(target_user):\n",
      "    personalization = { k : 0 for k in G.nodes() }\n",
      "    personalization[to_usr(target_user)] = 1.0\n",
      "    pr = nx.pagerank_scipy(G, personalization = personalization, max_iter=100)\n",
      "    \n",
      "    already_seen = set( review_df [ review_df.user_id == target_user ]['business_id'] )\n",
      "    new_biz = sorted( [(k[1],v) for k,v in pr.iteritems() if v>1e-3 and k[0]=='b' and k[1] not in already_seen],\n",
      "                     key = lambda x: x[1], \n",
      "                     reverse = True)\n",
      "\n",
      "    map_biz_by_id( map(lambda x: x[0], new_biz[:15]) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.html.widgets import interact\n",
      "import random\n",
      "\n",
      "interact(map_user_recs, target_user=[u for u in usr if random.random() < 0.0003])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2014 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}