{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Spark MLlib\n",
      "*Official documentation [here](https://spark.apache.org/docs/latest/mllib-guide.html).*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### MLlib Data Types\n",
      "**Vector**  \n",
      "A mathematical vector. MLlib supports both dense vectors, where every entry is stored, and sparse vectors, where only the nonzero entries are stored to save space. Vectors can be constructed with the mllib.linalg.Vectors class.\n",
      "\n",
      "```scala\n",
      "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
      "\n",
      "// Create a dense vector (1.0, 0.0, 3.0).\n",
      "val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)\n",
      "// Create a sparse vector (1.0, 0.0, 3.0) by specifying its indices and values corresponding to nonzero entries.\n",
      "val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))\n",
      "// Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries.\n",
      "val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))\n",
      "```\n",
      "**LabeledPoint**  \n",
      "A labeled data point for supervised learning algorithms such as classification and regression. Includes a feature vector and a label (which is a floating-point value). Located in the mllib.regression package.\n",
      "```scala\n",
      "scala> var a = data.first()\n",
      "a: org.apache.spark.mllib.regression.LabeledPoint = (-1.0,(8,[0,1,2,3,4,5,6,7],[6.0,148.0,72.0,35.0,0.0,33.599998,0.627,50.0]))\n",
      "\n",
      "scala> a.label\n",
      "res0: Double = -1.0\n",
      "\n",
      "scala> a.features\n",
      "res1: org.apache.spark.mllib.linalg.Vector = (8,[0,1,2,3,4,5,6,7],[6.0,148.0,72.0,35.0,0.0,33.599998,0.627,50.0])\n",
      "```\n",
      "\n",
      "#### Local matrix\n",
      "Integer-typed row and column indices and double-typed values, stored on a single machine. \n",
      "\n",
      "```scala\n",
      "import org.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
      "\n",
      "// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
      "val dm: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))\n",
      "```\n",
      "\n",
      "#### Distributed matrix \n",
      "Long-typed row and column indices and double-typed values, stored distributively in one or more RDDs.\n",
      "\n",
      "- **RowMatrix:** Row-oriented distributed matrix without meaningful row indices, e.g., a collection of feature vectors. It is backed by an RDD of its rows, where each row is a local vector. We assume that the number of columns is not huge for a RowMatrix so that a single local vector can be reasonably communicated to the driver and can also be stored / operated on using a single node. \n",
      "- **BlockMatrix:** A distributed matrix backed by an RDD of MatrixBlocks, where a MatrixBlock is a tuple of ((Int, Int), Matrix), where the (Int, Int) is the index of the block, and Matrix is the sub-matrix at the given index with size rowsPerBlock x colsPerBlock\n",
      "- **IndexedRowMatrix:** Similar to a RowMatrix but with meaningful row indices. It is backed by an RDD of indexed rows, so that each row is represented by its index (long-typed) and a local vector.\n",
      "\n",
      "**Rating**  \n",
      "A rating of a product by a user, used in the `mllib.recommendation` package for product recommendation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training and Prediction \n",
      "\n",
      "An aside: [`spark.ml`](https://spark.apache.org/docs/latest/ml-guide.html) is an interface to mimic scikit-learn and is an alpha component as of Spark 1.2.\n",
      "\n",
      "\n",
      "\n",
      "#### Models \n",
      "- Supports linear models (linear and logistic regressions, SVMs), decision trees, ensembles of trees, clustering, etc.)\n",
      "- Each Model is the result of a training algorithm, and typically has a predict() method for applying the model to a new data point or to an RDD of new data points.\n",
      "\n",
      "#### Cross-validation\n",
      "\n",
      "1. Split your `data` RDD into training, cross-validation, and test\n",
      "```scala\n",
      "val Array(trainData, cvData, testData) = data.randomSplit(Array(0.8, 0.1, 0.1))\n",
      "trainData.cache()\n",
      "cvData.cache()\n",
      "testData.cache()\n",
      "```\n",
      "\n",
      "1. K-fold cross-validation  \n",
      "MlLib doesn't have explicit support for this except with its experimental `MLUtils.kFold()` helper function, which partitions your dataset for you into $k$ train/test splits at which point you can build $k$ different models and aggregate the results.\n",
      "\n",
      "From the docs:\n",
      "```scala\n",
      "def\n",
      "kFold[T](rdd: RDD[T], numFolds: Int, seed: Int)(implicit arg0: ClassTag[T]) : Array[(RDD[T], RDD[T])]\n",
      "```\n",
      "Returns a k element array of pairs of RDDs with the first element of each pair containing (training data, a complement of the validation data) and the second element, the validation data, containing a unique 1/kth of the data.\n",
      "\n",
      "Here is some example code that you can emulate (see the KFold class in projects/spark_ml):\n",
      "```scala\n",
      "// Load some example pairs\n",
      "val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
      "\n",
      "// (train, test) data rdd pairs\n",
      "val folds = MLUtils.kFold(data, 10, 11)\n",
      "\n",
      "// (model, test) data rdd pairs\n",
      "val models = folds.map{ case (train, test) => \n",
      "\t(new LogisticRegressionWithLBFGS().setNumClasses(10).run(train), test) }\n",
      "\n",
      "// compute raw scores on the test set \n",
      "val modelPredictionAndLabels = models.map{ case (model, test) => \n",
      "\t(model, test.map { case LabeledPoint(label, features) => \n",
      "\t\tval prediction = model.predict(features)\n",
      "\t\t(prediction, label) } ) }\n",
      "\n",
      "//get metrics\n",
      "val precisions = modelPredictionAndLabels.map{ case (model, predictionLabels) => { \n",
      "\tval metrics = new MulticlassMetrics(predictionLabels) \n",
      "\tval precision = metrics.precision\n",
      "\tprecision\n",
      "\t}\n",
      "}\n",
      "```\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### A digression: Plotting (in-memory) with Spark and Scala\n",
      "\n",
      "#### Adding  external dependencies\n",
      "TL;DR use the `--packages` flag for external dependencies with Maven coordinates and `--jars` flag for local dependency jars for anything *not* in your `build.sbt`.\n",
      "\n",
      "- **spark-shell**: 3rd party (maven) dependencies in spark-shell with maven coordinates: \n",
      "```bash\n",
      "$ spark-shell --master {host} --packages \"{group}:{artifact}:{version}\" --jars oneJar.jar,anotherJar.jar\"\n",
      "```\n",
      "\n",
      "- **spark-submit**: Include 3rd party library jars to spark-submit. Note that packages with maven coordinates should properly be in your `build.sbt` if you are using `spark-submit` :\n",
      "```bash\n",
      "$ spark-submit --master {host} --jars {someJar}.jar,{anotherJar}.jar --class MyApp path/to/MyApp.jar\n",
      "```\n",
      "\n",
      "\n",
      "#### Example 1: plot values in spark-shell\n",
      "\n",
      "**N.B.** To get plots to appear from digital ocean on your local machine, you have to turn on X11 forwarding by ssh'ing into your box with `ssh -X`\n",
      "\n",
      "- To add dependencies in spark-shell with maven coordinates: \n",
      "```bash\n",
      "$ spark-shell --master local[4] --packages \"{group}:{artifact}:{version}\"\n",
      "```\n",
      "\n",
      "\n",
      "1. Fire up the spark-shell:\n",
      "```bash\n",
      "$ spark-shell --master local[*] --packages \"org.scalanlp:breeze-viz_2.10:0.11.2\"\n",
      "```\n",
      "\n",
      "1. Create a plot that appears in a separate window\n",
      "\n",
      "```scala\n",
      "scala> val a = Array(0.3, 0.9, 1.4, 34.21, -3.4)\n",
      "a: Array[Double] = Array(0.3, 0.9, 1.4, 34.21, -3.4)\n",
      "\n",
      "scala> import breeze.plot._\n",
      "import breeze.plot._\n",
      "\n",
      "scala> val f = Figure()\n",
      "f: breeze.plot.Figure = breeze.plot.Figure@73543048\n",
      "\n",
      "scala> val p = f.subplot(0)\n",
      "p: breeze.plot.Plot = breeze.plot.Plot@2e549515\n",
      "\n",
      "scala> p += plot(a,a,'.')\n",
      "res4: breeze.plot.Plot = breeze.plot.Plot@2e549515\n",
      "\n",
      "scala> val g = breeze.stats.distributions.Gaussian(0,1)\n",
      "...\n",
      "scala> val p2 = f.subplot(1,1)\n",
      "...\n",
      "scala> p2 += hist(g.sample(100000),100)\n",
      "```\n",
      "\n",
      "#### Example 2: plot a distribution\n",
      "\n",
      "Check out Cloudera's [`KernelDensity.estimate` implementation](https://github.com/sryza/aas/tree/master/ch09-risk/src/main/scala/com/cloudera/datascience/risk)\n",
      "\n",
      "```scala\n",
      "import breeze.plot._\n",
      "import com.cloudera.datascience.risk._\n",
      "\n",
      "object AnalysisClass {\n",
      "    ...\n",
      "  def plotDistribution(samples: Array[Double]): Figure = {\n",
      "    val min = samples.min\n",
      "    val max = samples.max\n",
      "    // Using toList before toArray avoids a Scala bug\n",
      "    val domain = Range.Double(min, max, (max - min) / 100).toList.toArray\n",
      "    val densities = KernelDensity.estimate(samples, domain)\n",
      "    val f = Figure()\n",
      "    val p = f.subplot(0)\n",
      "    p += plot(domain, densities)\n",
      "    p.ylabel = \"Density\"\n",
      "    f\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "In order to run this, include the *breeze* dependency in your `build.sbt`, and include the path to the `com.cloudera.datascience` package in the `--jars` flag of spark-submit.\n",
      "\n",
      "```scala\n",
      "libraryDependencies  ++= Seq(\n",
      "  \"org.scalanlp\" %% \"breeze\" % \"0.11.2\",\n",
      "  \"org.scalanlp\" %% \"breeze-natives\" % \"0.11.2\"\n",
      ")\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Use SVM to predict colon cancer from gene expressions\n",
      "You can start getting a feel for the MLlib operations by following the [Spark docs example](https://spark.apache.org/docs/1.3.0/mllib-linear-methods.html#linear-support-vector-machines-svms) on this dataset.\n",
      "\n",
      "#### About the data format: LibSVM\n",
      "MLLib conveniently provides a data loading method, `MLUtils.loadLibSVMFile()`, for the [LibSVM format](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#colon-cancer) for which many other languages (R, Matlab, etc.) also have loading methods.  \n",
      "A dataset of *n* features will have one row per datum, with the label and values of each feature organized as follows:\n",
      ">{label} 1:{value} 2:{value} ... n:{value}\n",
      "\n",
      "Take these two datapoints with six features and labels of -1 and 1 respectively as an example:\n",
      ">-1.000000  1:2.080750 2:1.099070 3:0.927763 4:1.029080 5:-0.130763 6:1.265460  \n",
      "1.000000  1:1.109460 2:0.786453 3:0.445560 4:-0.146323 5:-0.996316 6:0.555759 \n",
      "\n",
      "#### About the colon-cancer dataset\n",
      "This dataset was introduced in the 1999 paper [Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays.](http://www.pnas.org/content/96/12/6745.short)  \n",
      "\n",
      "Here's the abstract of the paper:  \n",
      "> *Oligonucleotide arrays can provide a broad picture of the state of the cell, by monitoring the expression level of thousands of genes at the same time. It is of interest to develop techniques for extracting useful information from the resulting data sets. Here we report the application of a two-way clustering method for analyzing a data set consisting of the expression patterns of different cell types. Gene expression in 40 tumor and 22 normal colon tissue samples was analyzed with an Affymetrix oligonucleotide array complementary to more than 6,500 human genes. An efficient two-way clustering algorithm was applied to both the genes and the tissues, revealing broad coherent patterns that suggest a high degree of organization underlying gene expression in these tissues. Coregulated families of genes clustered together, as demonstrated for the ribosomal proteins. Clustering also separated cancerous from noncancerous tissue and cell lines from in vivo tissues on the basis of subtle distributed patterns of genes even when expression of individual genes varied only slightly between the tissues. Two-way clustering thus may be of use both in classifying genes into functional groups and in classifying tissues based on gene expression.*\n",
      "\n",
      "There are 2000 features, 62 data points (40 tumor (label=0), 22 normal (label=1)), and 2 classes (labels) for the colon cancer dataset. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}