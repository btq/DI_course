{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Using Amazon's Cloud Services\n",
      "\n",
      "Amazon Web Services has extremely thorough documentation around everything from the commands available to the command line interface (CLI) `aws {commands}`, to the Python wrapper for said interface `boto`, to full tutorials and examples on how to fire up an EMR cluster or a bunch of EC2 instances with almost any desired data processing framework.\n",
      "\n",
      "EC2 is cheaper than EMR, but EMR is recommended for immediate use of Hadoop and any other project in the ecosystem (configurable for your cluster via [Amazon Machine Images](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/ami-versions-supported.html) (AMIs). In a production setting it's possible you'll want to use specific versions for consistency; in our case it's safe to use the most recent version (`3.6.0` at the time of this writing).\n",
      "\n",
      "## Getting started: set up AWS tools\n",
      "### AWS credentials and command line tools\n",
      "\n",
      "1. Copy the credentials into place:\n",
      "```\n",
      "mkdir ~/.aws\n",
      "cp ~/datacourse/secrets/credentials.sample ~/.aws/credentials\n",
      "cp ~/datacourse/secrets/mrjob.conf.sample ~/.mrjob.conf\n",
      "```\n",
      "1. Open them and store the value of the secret key into **both files**. \n",
      "\n",
      "1. Make sure the `*.pem` file referenced in `~/.mrjob.conf` is on your machine.\n",
      "\n",
      "1. Download the [AWS Command Line Interface (AWS CLI)](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html).  On your command line, type (possibly needing `sudo`)\n",
      "```\n",
      "pip install awscli\n",
      "```\n",
      "To verify that it is working, try \n",
      "```\n",
      "aws s3 ls\n",
      "```\n",
      "You should get back a json blob that contains the permissions you just added for your user.  If not, double-check that you got your permissions setup correctly.\n",
      "\n",
      "1. `boto` ([docs](https://boto.readthedocs.org/en/latest/)) is a python library that wraps the functionality of `awscli`.  You can install it using\n",
      "```\n",
      "pip install boto\n",
      "```\n",
      "and follow the instructions in the docs to get started.\n",
      "\n",
      "1. A better option for interacting with s3 from the command line is `s3cmd`. You can download/start using it via   \n",
      "`git clone https://github.com/s3tools/s3cmd.git` and follow the documentation [here](https://github.com/s3tools/s3cmd).\n",
      "\n",
      "### Python `mrjob`\n",
      "\n",
      "You need to now setup mrjob (it's already installed on the servers).  You can install `mrjob` via\n",
      "```\n",
      "pip install mrjob\n",
      "```\n",
      "To test it, clone this [github repo](https://github.com/Yelp/mrjob) and run wordcount on README.rst:\n",
      "```bash\n",
      "git clone https://github.com/Yelp/mrjob.git\n",
      "cd mrjob\n",
      "# run command locally\n",
      "# this is good for testing and debugging on files locally\n",
      "python examples/mr_word_freq_count.py README.rst\\\n",
      "   --no-output --output-dir=/tmp/wc/\n",
      "   \n",
      "# check the output file contents:\n",
      "cat /tmp/wc/* | more\n",
      "\n",
      "# run command on ec2 and write output to our s3 bucket\n",
      "# this costs money so only do it when you have working code\n",
      "python examples/mr_word_freq_count.py -r emr README.rst \\\n",
      "   --no-output --output-dir=s3://thedataincubator-fellow/<user>-wc/\n",
      "   \n",
      "# check the output file contents:\n",
      "aws s3 ls s3://thedataincubator-fellow/<user>-wc/\n",
      "aws s3 cp --recursive s3://thedataincubator-fellow/<user>-wc/ /tmp/<user>-wc/\n",
      "```\n",
      "Note: be sure to fill in `<user>` with a key that is unique to you.\n",
      "\n",
      "A few notes:\n",
      "1. You can also upload files to s3 using the AWS CLI so that your entire workflow can be on s3.\n",
      "1. The server will take a while to boot up the first time but it will stay alive.  Any subsequent jobs that are submitted will not have to reboot.  If there are already jobs running, it will wait up to 5 minutes before spawning another server (please be patient).  It will stay idle for 2 hours and then kill itself.\n",
      "1. Take a look at `examples/mr_word_freq_count.py`.  This is the simple \"word count\" mapreduce.\n",
      "\n",
      "### Useful AWS commands\n",
      "\n",
      "- Access Hadoop web UI, e.g. ResourceManager\n",
      "    - Option 1: via a local port\n",
      "```bash \n",
      "ssh -L 8158:ec2-52-0-25-37.compute-1.amazonaws.com:9026 hadoop@ec2-52-0-25-37.compute-1.amazonaws.com -i /path/to/fellos201501.pem\n",
      "```\n",
      "Now in your browser, go to the address `localhost:8158`  \n",
      "\n",
      "    - Option 2: via dynamic port forwarding\n",
      "        1. Type: \n",
      "```bash\n",
      "aws emr socks --cluster-id j-XXXX --key-pair-file ~/path/to/keypair.pem\n",
      "```\n",
      "        1. Then follow [these steps)(http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-connect-master-node-proxy.html) to add FoxyProxy to your Chrome/Firefox browser to natively use the web UI.\n",
      "\n",
      "\n",
      "- `ssh` into master node to access HDFS, Pig interactive, etc.\n",
      "\n",
      "```bash\n",
      "ssh hadoop@{master-public-dns}.compute-1.amazonaws.com -i /path/to/pemfile.pem\n",
      "```\n",
      "\n",
      "- One-liner to preview a .gz file:\n",
      "```bash\n",
      "s3cmd get s3://thedataincubator-course/wikidata/wikistats/pagecounts/pagecounts-20081208-030000.gz - | gunzip -c | less\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Spark on EC2\n",
      "Spark comes with a built-in script to launch clusters on Amazon EC2. This script launches a set of nodes and then installs the Standalone cluster manager on them.   \n",
      "Cloudera [recommends running on YARN](http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/) for production clusters. Since we'll only be using Spark in isolation (i.e. without any interaction and IO with other frameworks like MapReduce and Impala), standalone is fine; however there is currently better documentation for configuring/tuning Spark jobs running on YARN.\n",
      "\n",
      "\n",
      "To launch 5 m3.xlarge slave nodes, from `$SPARK_HOME/ec2` run:\n",
      "```bash\n",
      "$ ./spark-ec2 --key-pair=springfellows2015 --identity-file=~/springfellows2015.pem -s 5 --instance-type=m3.xlarge --region=us-east-1 --zone=us-east-1a --copy-aws-credentials launch myclustername\n",
      "```\n",
      "### Spark on EMR \n",
      "If you want to use Spark-on-Yarn, i.e. spin up a Hadoop cluster from which you can run multiple frameworks, you can install Spark and configure the other installed applications when you use the CLI to create a EMR cluster.  \n",
      "Here we are installing Impala, Hive, Pig, and Spark on one m3.xlarge instance; then running one of the example Spark jobs packaged with Spark (calculating Pi):\n",
      "```bash\n",
      "aws emr create-cluster --name EMR-Spark-Step-Example --ami-version 3.6 \\\n",
      "--instance-type=m3.2xlarge --instance-count 3 --applications Name=Hive,Impala,Pig  \\\n",
      "--use-default-roles --ec2-attributes KeyName=<YOUR_EC2_KEY_NAME>, \\\n",
      "--log-uri s3://thedataincubator-fellow/logs/ \\\n",
      "--bootstrap-action Name=Spark,Path=s3://support.elasticmapreduce/spark/install-spark,Args=[-x] \\\n",
      "--steps Name=SparkPi,Jar=s3://<REGION_OF_CLUSTER>.elasticmapreduce/libs/script-runner/script-runner.jar,Args=[/home/hadoop/spark/bin/spark-submit,--deploy-mode,cluster,--master,yarn-cluster,--class,org.apache.spark.examples.SparkPi,s3://support.elasticmapreduce/spark/1.2.0/spark-examples-1.2.0-hadoop2.4.0.jar,10] \\\n",
      "--auto-terminate\n",
      "```\n",
      "\n",
      "We'll be using a publicly available dataset from S3 of Wikipedia site traffic to demonstrate typical ETL tasks under different frameworks. We'll only directly compare two (Spark and Scalding), but feel free to fire up a cluster and use Amazon's ample documentation to learn how to use some of the other most popular frameworks, e.g. Pig, Impala, and Crunch."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}