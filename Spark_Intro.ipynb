{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# What is Spark?\n",
      "Like _Hadoop_, __Spark__ is a low-level system for distributed computation on a cluster.  It has two major advantages over MapReduce:\n",
      " \n",
      " - It can do in-memory caching between stages, while Hadoop must write everything to disk.  This improves performance.  It also makes it suitable for classes of algorithms that would otherwise be too slow (e.g., iterative algorithms, like the training step in many machine learning algorithms).\n",
      " \n",
      " - It has a more flexible execution model (i.e., not just MapReduce).\n",
      " \n",
      "There are also minor advantages: The default API is much nicer than writing raw MR code. The favored interface is via a language called Scala, but there is also good support for using Python.\n",
      "## How does Spark relate to Hadoop?  \n",
      "\n",
      "__Answer 1__: It's a replacement for it.  You can manage and run your own Spark cluster, independent of Hadoop.  You'll have to figure out the filesystem layer from scratch though.   (In this context, _Shark_ is the replacement for _Hive_, i.e., for SQL-like operation.)\n",
      "\n",
      "__Answer 2__: It's a complement to it.  You can run Spark on top of a Hadoop cluster, and still leverage HDFS and YARN -- then Spark is just replacing MapReduce.  (In this context, _Shark_ can be used as a drop-in replacement for _Hive_.)\n",
      "\n",
      "## The Spark API\n",
      "### Nouns\n",
      "The two main abstractions in Spark are:\n",
      "1. Resilient distributed datasets (RDDs):\n",
      "  This is like a (smartly) distributed list, which is partitioned across the nodes of the cluster and can be operated on in parallel.  The operations take the form of the usual functional list operations as shown above.  There are two types of operations: Transformations and Actions.\n",
      "1. Shared variables used in parallel operations: When Spark runs a function in parallel across nodes, it ships a copy of each variable needed by that function to each node.  There are two types of shared variables:\n",
      " - Broadcast variables: which are used to store a value in memory across all nodes (i.e., their values are \"broadcast\" from the driver node)\n",
      " - Accumulator variables: these are variables which are only added to across nodes, for implementing counters and sums.\n",
      " \n",
      "### Verbs\n",
      "- [Transformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations): This creates a new RDD from an old one, for instance `map` or `flatMap`.  Transformations in Spark are __lazy__, this means that they do not compute the answer right away -- only when it is needed by something else.  Instead they represent the \"idea\" of the transformation applied to the base data set (e.g., for each chaining).  In particular, this means that intermediate results to computations do not necessarily get created and stored -- if the output of your map is going straight into a reduce, Spark should be clever enough to realize this and not actually store the output of the map.  Another consequence of this design is that the results of transformations are, by default, recomputed each time they are needed -- to store them one must explicitly call `cache`.\n",
      "- [Actions](http://spark.apache.org/docs/latest/programming-guide.html#actions): These actually return a value as a result of a computation.  For instance, `reduce` is an action that combines all elements of an RDD using some function and then returns the final result.  (Note that `reduceByKey` actually returns an RDD.)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Why would you *not* use Spark?\n",
      "A large reason for why MapReduce is so powerful is that it abstracts the problem of programming/utilizing a large network of machines away from the programmer. By and large, a MapReduce application you write that works in standalone (local) mode will scale. (This especially holds true for the MapReduce interfacs like Pig, Crunch, and Cascading that optimize resources for your job.)\n",
      "\n",
      "At the time of this writing, the state of Spark necessarily involves a lower level understanding of cluster resource management. For the newbie programmer or data scientist, this can be daunting."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Some common Transformations and Actions\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "|Transformation|Meaning|\n",
      "| ------------- |:----:|\n",
      "|map(func)|Return a new distributed dataset formed by passing each element of the source through a function func.|\n",
      "|filter(func)|Return a new dataset formed by selecting those elements of the source on which func returns true.|\n",
      "|flatMap(func)|Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).|\n",
      "|mapPartitions(func)|Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type `Iterator<T> => Iterator<U>` when running on an RDD of type T.|\n",
      "|mapPartitionsWithIndex(func)|Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type `(Int, Iterator<T>) => Iterator<U>` when running on an RDD of type T.|\n",
      "|sample(withReplacement, fraction, seed)|Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.|\n",
      "|union(otherDataset)|Return a new dataset that contains the union of the elements in the source dataset and the argument.|\n",
      "|intersection(otherDataset)|Return a new RDD that contains the intersection of elements in the source dataset and the argument.|\n",
      "|distinct([numTasks]))|Return a new dataset that contains the distinct elements of the source dataset.|\n",
      "|groupByKey([numTasks])|When called on a dataset of (K, V) pairs, returns a dataset of `(K, Iterable<V>)` pairs. **Note**: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance. **Note:** By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.|\n",
      "|reduceByKey(func, [numTasks])|When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.|\n",
      "|aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])|When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.|\n",
      "|sortByKey([ascending], [numTasks])|When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.|\n",
      "|join(otherDataset, [numTasks])|When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.|\n",
      "|cogroup(otherDataset, [numTasks])|When called on datasets of type (K, V) and (K, W), returns a dataset of `(K, (Iterable<V>, Iterable<W>))` tuples. This operation is also called groupWith.|\n",
      "|cartesian(otherDataset)|When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).|\n",
      "|pipe(command, [envVars])|Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.|\n",
      "|coalesce(numPartitions)|Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.|\n",
      "|repartition(numPartitions)|Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.|\n",
      "|repartitionAndSortWithinPartitions(partitioner)|Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery.|"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "|Action|Meaning|\n",
      "|------|:------|\n",
      "|reduce(func)|Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.|\n",
      "|collect()|Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.|\n",
      "|count()|Return the number of elements in the dataset.|\n",
      "|first()|Return the first element of the dataset (similar to take(1)).|\n",
      "|take(n)|Return an array with the first n elements of the dataset.|\n",
      "|takeSample(withReplacement, num, [seed])|Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.|\n",
      "|takeOrdered(n, [ordering])|Return the first n elements of the RDD using either their natural order or a custom comparator.|\n",
      "|saveAsTextFile(path)|Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.|\n",
      "|saveAsSequenceFile(path) |\n",
      "|(Java and Scala)|Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that either implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).|\n",
      "|saveAsObjectFile(path) |\n",
      "|(Java and Scala)|Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile().|\n",
      "|countByKey()|Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.|\n",
      "|foreach(func)|Run a function func on each element of the dataset. This is usually done for side effects such as updating an accumulator variable (see below) or interacting with external storage systems.|"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### A primer on \"functional\" style for list processing\n",
      "Spark\u2019s API relies heavily on passing functions in the driver program to run on the cluster. It inherits this style of functional programming by virtue of being written in Scala, a language built on the JVM that fuses functional and object-oriented programming in a practical package. \n",
      "\n",
      "There are two ways you can do this: by anonymous functions and by static methods. \n",
      "\n",
      "#### In Scala\n",
      "\n",
      "- Map every String in my RDD to another String\n",
      "\n",
      "```scala\n",
      "def func1(s: String): String = { ... }\n",
      "myRdd.map(func1)\n",
      "```\n",
      "- Sort my `boolean_count_rdd` PairRDD by the first value in the tuple (default is ascending) and print each tuple:\n",
      "\n",
      "```scala\n",
      "boolean_count_rdd.sortBy(_._1).foreach(println)  \n",
      "\n",
      "...\n",
      "\n",
      "(false,5728201)\n",
      "(true,20931)\n",
      "```\n",
      "\n",
      "#### In Python\n",
      "It is Pythonic to operate on lists -- elementwise operations, filtering, etc. -- by using list comprehensions.  In many other languages, starting with Lisp but extending to many \"functional\" programming languages, a different style is preferred:\n",
      "\n",
      "The idea is that if `f` is a function, then one thinks of the application\n",
      ">          \n",
      "    list   |---->   [ f(x) for x in list ]\n",
      "\n",
      "on lists as a function of _two_ arguments: `f` and `list`.  The idea of viewing the function `f` as a parameter is typical in functional programming languages, and can be taken as a definition of the later term.\n",
      "\n",
      "Some common idioms in this style, with Pythonic equivalents, are:\n",
      "\n",
      "- `map(f, list) === [ f(x) for x in list ]`: Apply `f` element-wise to `list`.\n",
      "- `filter(f, list) === [ x for x in list if f(x) ]`: Filter `list` using `f`.\n",
      "- `flatMap(f, list) === [ f(x) for y in list for x in y ]`: Here `f` is a function that eats elements (of the type contained in list) and spits out lists, and `flatMap` first applies f element-wise to the elements of `list` and then _flattens_ or _concatenates_ the resulting lists.  It is sometimes also called `concatMap`.\n",
      "- `reduce(f, list[, initial])`: Here `f` is a function of _two_ variables, and folds over the list applying `f` to the \"accumulator\" and the next value in the list.  That is, it performs the following recursion\n",
      "\n",
      "$$    a_{-1} = \\mathrm{initial} $$\n",
      "$$    a_i = f(a_{i-1}, \\mathrm{list}_i) $$\n",
      "\n",
      "with the with the final answer being $a_{\\mathrm{len}(\\mathrm{list})-1}$.  (If initial is omitted, just start with $a_0 = \\mathrm{list}_0$.)  For instance,\n",
      ">           \n",
      "    reduce(lambda x,y: x+y, [1,2,3,4]) = ((1+2)+3)+4 = 10\n",
      "    \n",
      "    \n",
      "###Remark:\n",
      "This is where the name \"map reduce\" comes from.."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### RDD Persistence\n",
      "When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.\n",
      "\n",
      "Two options for RDD persistence:\n",
      "- Cache `my_rdd` in memory (deserialized)\n",
      "```scala\n",
      "my_rdd.cache()\n",
      "```\n",
      "- Cache `my_rdd` as serialized Java objects in memory: more space-efficient than deserialized objects, but more CPU-intensive to read \n",
      "```scala\n",
      "my_rdd.persist(MEMORY_ONLY_SER)\n",
      "```\n",
      "\n",
      "*Serialization*: the process of translating data structures or object state into a format (i.e. series of bits) that can be stored and reconstructed later. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Why Scala?\n",
      "[Scala For The Impatient](http://logic.cse.unt.edu/tarau/teaching/scala_docs/scala-for-the-impatient.pdf)\n",
      "\n",
      "1. It reduces performance overhead\n",
      "2. Access to latest and greatest\n",
      "3. Understand the underlying philosophy of computation that Spark inherits from being developed in Scala\n",
      "4. One language to rule them all:\n",
      "> With Spark and Scala, the experience is different, because you\u2019re using the same language for everything. You\u2019re writing Scala to retrieve data from the cluster via Spark. You\u2019re writing Scala to manipulate that data locally on your own machine. And then \u2014 and this is the really neat part \u2014 you can send Scala code into the cluster so that you can perform the exact same transformations that you performed locally on data that is still stored in the cluster. It\u2019s difficult to express how transformative it is to do all of your data munging and analysis in a single environment, regardless of where the data itself is stored and processed. It\u2019s the sort of thing that you have to experience for yourself to understand, and we wanted to be sure that our recipes captured some of that same magic feeling that we felt when we first started using Spark."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## In-class exercise: beginning Spark ETL\n",
      "### Premise\n",
      "The first steps of any data science project usually involves ETL'ing and performing exploratory analytics on a dataset.\n",
      "Here we'll acquaint ourselves both with the Spark shell and some Spark functions that we can use to perform such tasks.\n",
      "\n",
      "### Exercise\n",
      "\n",
      "Fire up the spark shell by entering at the command line,\n",
      "```bash\n",
      "spark-shell \n",
      "```\n",
      "Load in the dataset with:\n",
      "```scala\n",
      "val pathToData = \"...\"\n",
      "val lines = sc.textFile(pathToData)\n",
      "```\n",
      "\n",
      "#### The dataset\n",
      "From https://archive.ics.uci.edu/ml/datasets/EEG+Database:  \n",
      "> \"The first four lines are header information. Line 1 contains the subject identifier and indicates if the subject was an alcholic (a) or control (c) subject by the fourth letter. Line 4 identifies the matching conditions: a single object shown (S1 obj), object 2 shown in a matching condition (S2 match), and object 2 shown in a non matching condition (S2 nomatch). Line 5 identifies the start of the data from sensor FP1. The four columns of data are: the trial number, sensor position, sample number (0-255), and sensor value (in micro volts).\"\n",
      "\n",
      "There are 16,452 rows in this file including the header. Here's a preview of the first 10 lines and last 10 lines:\n",
      "```\n",
      "# co2a0000364.rd\n",
      "# 120 trials, 64 chans, 416 samples 368 post_stim samples\n",
      "# 3.906000 msecs uV\n",
      "# S1 obj , trial 0\n",
      "# FP1 chan 0\n",
      "0 FP1 0 -8.921\n",
      "0 FP1 1 -8.433\n",
      "0 FP1 2 -2.574\n",
      "0 FP1 3 5.239\n",
      "0 FP1 4 11.587\n",
      "    ...\n",
      "0 Y 246 24.150\n",
      "0 Y 247 20.243\n",
      "0 Y 248 11.454\n",
      "0 Y 249 4.618\n",
      "0 Y 250 3.153\n",
      "0 Y 251 6.571\n",
      "0 Y 252 12.431\n",
      "0 Y 253 15.849\n",
      "0 Y 254 16.337\n",
      "0 Y 255 14.872\n",
      "```\n",
      "\n",
      "#### ETL'ing the dataset\n",
      "\n",
      "Let's filter out the header first.\n",
      "```scala\n",
      "def isHeader(line: String) : Boolean = {\n",
      "    line.contains(\"# \")\n",
      "}\n",
      "```\n",
      "Next, let's parse each record line into an object of type `Record`, which we will define using Scala's case classes.\n",
      "```scala\n",
      "// case class: a simple type of immutable class that comes with\n",
      "// implementations of all the basic Java methods, e.g. toString, equals, hashCode\n",
      "case class Record(trial: Int, posn: String, sample: Int, reading: Double)\n",
      "\n",
      "def parse(line: String) = {\n",
      "    val tokens = line.split(\"\\\\s+\")\n",
      "    val trial = tokens(0).toInt\n",
      "    val posn = tokens(1)\n",
      "    val sample = tokens(2).toInt\n",
      "    val reading = tokens(3).toDouble\n",
      "    Record(trial, posn, sample, reading)\n",
      "}\n",
      "\n",
      "```\n",
      "\n",
      "#### Summary stats on the dataset\n",
      "\n",
      "1. There is an action for `RDD[Double]` called `stats` that will provide us with summary statistics about the values in the RDD. How can we get out summary stats of the `reading` column across the entire dataset?\n",
      "\n",
      "1. How can we do the same but for only the `posn` \"FP1\"?\n",
      "\n",
      "1. Let's make sure there are 256 samples per `posn` in this dataset. How can we do this? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}