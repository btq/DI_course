{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Model building. \"Econometric\" / forecasting case study: Predicting mortgage performance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goals / Topics\n",
      "-------------\n",
      " - Sub-models\n",
      " - Indicator variables\n",
      " - Discrete bins and splining\n",
      " - Common macroeconomic inputs\n",
      " - \"Epochs\" -- model design, testing, and limitations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##The problem: Predicting mortgage performance\n",
      "\n",
      "You work for a financial institution that buys and holds single-family mortgage debt.  You want to be able to accurately _price_ that mortgage debt.  Actually, we'll do something just a little bi easier:\n",
      "\n",
      "Suppose given a loan's financial characteristics at origination, and its monthly payment history up until now -- predict its monthly payment history in the future."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**The type of learner**: This will be a theoretical model, with several _supervised regression_ problems as inputs.\n",
      "\n",
      "**The training dataset**: There is loan-level loan origination and performance data available free of charge (but with use restrictions) from [Freddie Mac](http://www.freddiemac.com/news/finance/sf_loanlevel_dataset.html).  To make the data small enough so that we can experiment _quickly_ we'll make some further restrictions.\n",
      "\n",
      "**The test dataset**: We'll try to develop a holdout data set from our training data.  This will be surprisingly difficult, and we'll discuss the difficulties and _limitations_."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Most naive \"model\"\n",
      "\n",
      "Let's start with the simplest possible \"model.\"  We are going to ignore any information about the mortgage itself, and instead just use our general know-how about how mortgages work:\n",
      "\n",
      "\n",
      "###__Step 1: Assumptions__\n",
      " 1. Most home mortgages are 30-year fixed rate loans (and the Freddie data is only this).  Let $I$ denote the fixed annual interest rate, and the payment period of the loan consists of 360 monthly payments.\n",
      " 2. People generally try to make a fixed monthly payment on their mortgage (\"constant payment rate\") over the lifetime of the loan.\n",
      "\n",
      "The fixed payment in 2 is what a \"loan calculator\" formula would give you.  Let's figure out what it is, and once we've done that -- we'll have built our naive model.\n",
      "\n",
      "###__Step 2: Figure out the fixed payment amount.__\n",
      "\n",
      "Let $R$ denote the (constant) payment amount each month.  Let's derive the basic \"loan calculator\" formula for $R$ in terms of the original balance and $I$ (recall that the duration is fixed at $360$ months).\n",
      "\n",
      "For each $0 \\leq n \\leq 360$ let\n",
      " $$ \\mathrm{UPB}_n = \\text{the remaining unpaid balance on the loan at $n$ months past origination} $$\n",
      "\n",
      "so that\n",
      "\n",
      " $$ \\mathrm{UPB}_{n+1} = (1+\\tfrac{I}{12}) \\cdot \\mathrm{UPB}_{n} - R $$\n",
      "\n",
      "or \n",
      "\n",
      " $$ \\mathrm{UPB}_n = (1+\\tfrac{I}{12})^n UPB_0 - [(1+\\tfrac{I}{12})^{n-1} + \\cdots + 1] R = (1+\\tfrac{I}{12})^n UPB_0 - \\frac{(1+\\tfrac{I}{12})^n - 1}{(1+\\tfrac{I}{12})-1} R $$\n",
      "\n",
      "and solving for the final condition that $UPB_{360} = 0$ we obtain the formula behind loan calculators\n",
      "\n",
      " $$ R = (1+\\tfrac{I}{12})^{360} \\frac{(1+\\tfrac{I}{12})-1}{(1+\\tfrac{I}{12})^{360}-1} \\mathrm{UPB}_0 $$\n",
      "\n",
      "In this simple model the total amount that the borrower will pay is $360 R$.  For instance at an annual interest rate of $I = 0.05$ this works out to the borrower paying back approximately $1.93 \\cdot \\mathrm{UPB}_0$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing the naive model (/its assumptions)\n",
      "Before complicating things, let's understand how well the naive model worked:\n",
      "\n",
      "We will not question Assumption 1 -- we are going to declare by fiat that we are interested in fixed 30 year loans (this is, for instance, the only loan type included in the Freddie Mac data set).\n",
      "\n",
      "For Assumption 2, and thus the model itself -- we can \"test\" it by just graphing the errors.  (Since there was no training to speak of, we don't have to worry about cross valdation strategies yet.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlalchemy as sql\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import getpass\n",
      "\n",
      "pd.options.display.mpl_style = 'default'\n",
      "\n",
      "# The '_cut' data-base has been pre-processed to be much smaller -- it only contains data for Delaware\n",
      "engine = sql.create_engine(\"postgres://{user}@/freddie_cut\".format(user=getpass.getuser()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_orig = pd.read_sql(\"\"\"\n",
      "    SELECT loan_seqn, credit_score, msa, cltv, dti, orig_upb, ltv, orig_rate, \n",
      "            postcode, year, month FROM freddie_originations \n",
      "    WHERE postcode='19800'\n",
      "    \"\"\", engine)\n",
      "print df_orig.columns\n",
      "df_orig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_orig[['credit_score', 'dti', 'ltv', 'orig_rate']].hist(bins=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pre-processing in Pandas\n",
      "Let's do some pre-processing in pandas, adding in some extra fields to the data.\n",
      "\n",
      "In real life, we'd probably do this in SQL -- with the advantage of not having to worry about fitting in RAM / it being stored for later use / etc.\n",
      "\n",
      "(In fact, we even did it in this example.  This is for demonstration purposes only!)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Augment the originations data with the expected constant monthly payment\n",
      "\n",
      "# We're doing this step, and the ones immediately below, in pandas only as an example.\n",
      "#\n",
      "# If you look -- you'll see that we could also have done this in SQL, with the advantage of not\n",
      "# having to worry about fitting in RAM / it being stored for later use / etc.\n",
      "def pmt(upb, i, duration):\n",
      "    return upb * (1+i)**duration * ((1+i) - 1) / ((1+i)**duration - 1)\n",
      "\n",
      "def pmt_ann_30(upb, I):\n",
      "    return pmt(upb, I/12, 360)\n",
      "\n",
      "df_orig[\"Constant_Monthly_Payment\"] = pmt_ann_30(df_orig[\"orig_upb\"], df_orig[\"orig_rate\"]/100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_perf_by_origs(origs):\n",
      "    dfs = []\n",
      "    def get_perf(loan_seqn):\n",
      "        return pd.read_sql(\"  \".join([\"SELECT\",\n",
      "            \"p.loan_seqn, p.reporting_pd, p.current_upb, p.delinquency_status, p.loan_age, p.zero_balance_code\",\n",
      "            \"FROM freddie_performance p\",\n",
      "            \"WHERE p.loan_seqn='%s'\" % (loan_seqn,)]), engine)\n",
      "    return pd.concat( map(get_perf, origs) )\n",
      "\n",
      "df_perf = get_perf_by_origs(df_orig['loan_seqn'].values)\n",
      "#df_perf = pd.read_sql(\"SELECT p.loan_seqn, p.reporting_pd, p.current_upb, p.delinquency_status, p.loan_age, p.zero_balance_code \\\n",
      "#                      FROM \\\n",
      "#                        freddie_performance p, freddie_originations o \\\n",
      "#                        WHERE p.loan_seqn=o.loan_seqn\", engine)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_perf = df_perf.merge( df_orig[[\"loan_seqn\", \"Constant_Monthly_Payment\", \"orig_rate\", \"credit_score\", \"dti\", \"ltv\", \"year\", \"month\"]], on=\"loan_seqn\" )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add in columns for whether this month was a prepay / default event.\n",
      "def is_code_default(c):\n",
      "    if c == 97 or (2 <= c and c <= 8):\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "df_perf[\"Prepay\"] = df_perf.zero_balance_code.apply(lambda x: 1 if x==1 else 0)\n",
      "df_perf[\"Default\"] = df_perf.zero_balance_code.apply(lambda x: 1 if is_code_default(x) else 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Add in a \"amount paid\" column; a past delinquency column; and some origination data columns.\n",
      "def add_past_delinq(perf):\n",
      "    delinq_ser = perf.delinquency_status.values\n",
      "    found_age = 361\n",
      "    for i in range(len(delinq_ser)):\n",
      "        if delinq_ser[i] != \"0\":\n",
      "            found_age = perf.loan_age.values[i]\n",
      "            break\n",
      "    perf['past_delinq'] = map( lambda x: 1 if x >= found_age else 0, perf.loan_age )\n",
      "    return perf\n",
      "            \n",
      "df_perf = df_perf.groupby('loan_seqn').apply(add_past_delinq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_paid(perf):\n",
      "    perf['paid'] = (1 + perf['orig_rate']/100/12) * perf['current_upb'].shift(1) - perf['current_upb']\n",
      "    return perf\n",
      "\n",
      "df_perf = df_perf.groupby('loan_seqn').apply(add_paid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## The amount paid divided by the expected contant monthly payment\n",
      "diffs=(df_perf[ df_perf['zero_balance_code'].isnull() ]['paid'] / df_perf[ df_perf['zero_balance_code'].isnull() ]['Constant_Monthly_Payment'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print diffs.describe()\n",
      "diffs.hist(bins=50, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diffs[ abs(diffs) < 10 ].hist(bins=50, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diffs[ (0.5 < (diffs)) & ((diffs) < 1.5) ].hist(bins=50, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### So what?\n",
      "We have verified: The assumption that people make constant payments until the end is _very reasonable_."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Re-pulling the data from the DB\n",
      "Okay, that was fun.  I've already done the analogous processing in SQL though, so I'll re-pull the data from the database with one added twist --\n",
      "\n",
      "For reasons that will become apparent later, we will do two things\n",
      " - We will randomly sample the data (rather than just grabbing a zip code);\n",
      " - We will over-sample the (rare but important) defaults."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample_rate = 0.10\n",
      "oversample_defaults = 1\n",
      "\n",
      "df_orig = pd.read_sql(\"\"\"\n",
      "    SELECT loan_seqn, credit_score, msa, cltv, dti, \n",
      "            orig_upb, ltv, orig_rate, postcode, year, month, \n",
      "            constant_monthly_payment, ends_prepay, ends_default \n",
      "    FROM freddie_originations\n",
      "    WHERE random() < {sample_rate} * (1 + ({oversample_defaults} - 1)* (ends_default=1)::int)\n",
      "\"\"\".format(sample_rate = sample_rate, oversample_defaults = oversample_defaults), engine)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_perf_by_origs(origs):\n",
      "    orig_list = \",\".join( [\"'\" + o + \"'\" for o in origs] )\n",
      "    return pd.read_sql(\"\"\"\n",
      "        SELECT p.loan_seqn, p.reporting_pd, p.current_upb, p.delinquency_status, \n",
      "                p.loan_age, p.zero_balance_code, p.past_delinq, p.is_default, p.is_prepay, \n",
      "                o.constant_monthly_payment, o.orig_rate, o.credit_score, \n",
      "                o.dti, o.ltv, o.year, o.month , o.ends_default\n",
      "        FROM freddie_performance p \n",
      "        INNER JOIN freddie_originations o \n",
      "        ON p.loan_seqn = o.loan_seqn\n",
      "        WHERE p.loan_seqn IN ({0})\n",
      "        \"\"\".format(orig_list), engine) \n",
      "\n",
      "df_perf = get_perf_by_origs(df_orig['loan_seqn'].values)\n",
      "\n",
      "df_perf[\"weight\"] = (1-df_perf.ends_default) * 1.0 + df_perf.ends_default * 1.0 / oversample_defaults\n",
      "del df_perf[\"ends_default\"]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Modeling the default and prepay rates.\n",
      "What fraction of loans eventually prepay or default?  How long do they take to do so?  Does our data suffice to estimate this -- given thateven the oldest loans we have information on only have 14/30 years of performance data?\n",
      "\n",
      "We should qualitatively check all of these facts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Recall that we have purposefully over-sample the defaults by a factor of 10 (!)\n",
      "\n",
      "df_perf[df_perf.zero_balance_code.notnull()].groupby(df_perf.zero_balance_code).loan_seqn.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How long do loans go before they are \"done\"?\n",
      "# (This could be skewed by our sampling, but this graph is just for a rough idea\n",
      "#   -- we'll do a more detailed analysis below.)\n",
      "print df_perf[ df_perf['zero_balance_code'].notnull() ]['loan_age'].describe()\n",
      "df_perf[ df_perf['zero_balance_code'].notnull() ]['loan_age'].hist( bins = 50 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "good_duration = 7.5*12\n",
      "\n",
      "prepay_by_age = pd.DataFrame( index=np.arange(good_duration) )\n",
      "default_by_age = pd.DataFrame( index=np.arange(good_duration)  )\n",
      "remain_by_age = pd.DataFrame( index=np.arange(good_duration) )\n",
      "\n",
      "\n",
      "\n",
      "perft = df_perf[ (1999<=df_perf.year) & (df_perf.year<=2006) ]\n",
      "tot = len( df_orig[ (1999<=df_orig.year) & (df_orig.year<=2006) & (df_orig.ends_default==0) ]) + len( df_orig[ (1999<=df_orig.year) & (df_orig.year<=2006) & (df_orig.ends_default==1) ]) / oversample_defaults\n",
      "prepay_by_age[\"Total\"]  = perft[ perft.is_prepay == 1 ].groupby(perft.loan_age)['loan_age'].count()*1.0/tot\n",
      "default_by_age[\"Total\"] = perft[ perft.is_default == 1 ].groupby(perft.loan_age)['loan_age'].count()*1.0/tot/oversample_defaults\n",
      "    \n",
      "for year in range(1999, 2006+1):\n",
      "    perft = df_perf[ df_perf.year == year ]\n",
      "    tot = len(df_orig[ (df_orig.year == year) & (df_orig.ends_default==0) ]) + len(df_orig[ (df_orig.year == year) & (df_orig.ends_default==1) ]) / oversample_defaults\n",
      "    prepay_by_age[year]  = perft[ perft.is_prepay == 1 ].groupby(perft.loan_age)['loan_age'].count()*1.0/tot\n",
      "    default_by_age[year] = perft[ perft.is_default == 1 ].groupby(perft.loan_age)['loan_age'].count()*1.0/tot/oversample_defaults\n",
      "\n",
      "prepay_by_age_cum = prepay_by_age.cumsum().interpolate().fillna(0)\n",
      "default_by_age_cum = default_by_age.cumsum().interpolate().fillna(0)\n",
      "remain_by_age = 1 - prepay_by_age_cum - default_by_age_cum\n",
      "\n",
      "def draw_two(df, title):\n",
      "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "    df.drop('Total', axis=1).plot(ax=ax1)\n",
      "    plt.xlim(0, good_duration)\n",
      "    df['Total'].plot(ax=ax2)\n",
      "    plt.xlim(0, good_duration)\n",
      "    f.suptitle(title)\n",
      "    f.show()\n",
      "\n",
      "draw_two(prepay_by_age_cum, \"Cum. prepay rate by age (by vintage, and total)\")\n",
      "draw_two(default_by_age_cum, \"Cum. default rate by age (by vintage, and total)\")\n",
      "draw_two(remain_by_age, \"Fraction of loan remaining by age (by vintage, and total)\")\n",
      "\n",
      "window = 3\n",
      "cond_prepay = (pd.rolling_mean(prepay_by_age_cum.diff(), window=window)/pd.rolling_mean(remain_by_age, window=window))\n",
      "draw_two(cond_prepay, \"Smoothed conditional prepay rate by loan age (incl by vintage)\")\n",
      "\n",
      "cond_default = (pd.rolling_mean(default_by_age_cum.diff(), window=window)/pd.rolling_mean(remain_by_age, window=window))\n",
      "draw_two(cond_default, \"Smoothed conditional default rate by loan age (incl by vintage)\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "default_by_age = default_by_age_cum.diff().fillna(0)\n",
      "prepay_by_age = prepay_by_age_cum.diff().fillna(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats.mstats\n",
      "print 1-scipy.stats.mstats.gmean(map(lambda x: 1-x, prepay_by_age['Total']))\n",
      "print 1-scipy.stats.mstats.gmean(map(lambda x: 1-x, default_by_age['Total']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The arithmetic / geometric difference is insignificant.\n",
      "print prepay_by_age['Total'].mean()\n",
      "print default_by_age['Total'].mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pause for analysis:\n",
      "\n",
      "What do we see?  Well:\n",
      "\n",
      "- Most payments that are not (essentially) zero or paying off the loan are the constant payment amount.\n",
      "- The majority of loans terminate in \"pre-pays\" (zero_balance_code = 1).\n",
      "- Some small fraction of loans are delinquent for some number of months, and some fraction of those end up defaulting.  (zero_balance_code = 97, but we'll wrap up 2-8 into that as well for simplicity)\n",
      "- Nearly 3/4 of loans terminate within 6 years, and of those that remain a share of the balance has been paid off.  This means that prediction accuracy in later years is less important.\n",
      "- The peaks in the prepay curves aren't all in the same places -- more analysis reveals that they are around the same *times*, and presumably related to interest rate changes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Structuring the submodels\n",
      "\n",
      "What are we going to actually *do*: We want to build a model for the __conditional prepay and default rates__ that is\n",
      "\n",
      " 1. Default rates -- i.e., the chance borrower simply stops paying in a given month\n",
      "    $$ P(\\text{default in month $i$} \\; | \\; \\text{the loan is still active in month $(i-1)$}) $$\n",
      "\n",
      " 2. Prepayment rates -- i.e., the chance that the borrower pays off the loan early\n",
      "    $$ P(\\text{prepay in month $i$} \\; | \\; \\text{the loan is still active in month $(i-1)$}) $$\n",
      "\n",
      "\n",
      "These models have as input and output:\n",
      "\n",
      "**Input**: A loan's origination characteristics and age some aggregate statistics on past performance, current age, and current (known) performance history.\n",
      "\n",
      "**Output**: Two curves going month-by-month from the present out to when the loan is 360 months old: The cumulative default probability, and the cumulative prepay probability.\n",
      "\n",
      "If our loan is $n$ months old, we are thus trying to predict $2(360-n)$ numbers that are required to satisfy some constraints (each is between $0$ and $1$, there is a monotonicity requirement, etc.).\n",
      "\n",
      "There are several approaches to building this sort of model:\n",
      "\n",
      " - _Model the total default probability and pick a 'model shape' for attributing it over time_: For instance we might choose to make the default / prepay curves always be a __scalar multiple__ of the one obtained for the years 1999-2004 (and assumed flat or filled out in some reasonable way past the end of known data).  This reduces the problem to predicting just two numbers: the eventual default / prepay rate.\n",
      " - _Pick an analytic family of model shapes, and model the parameters_: Survival rates, like default and prepay rates here, are sometimes modelled according to a __gamma distribution__.  So, we would be fitting the parameters of a Gamma distribution -- and the output curve would be the cumulative distribution function of the fitted distribution.\n",
      " - _Spline_ the curve: e.g., assume that the default probability is piecewise constant (so the cumulative distribution is piecewise linear) and estimate the constant probability in each bucket. The extreme version of this is to build a _separate model for each age_.  But grouping things together means that you estimate things like \"probability of default in years 3-5\" instead of in a single month, which can help with data availability issues. \n",
      " - _Build a single model with age as a (non-linear) input_: If using a linear learner (like a logistic regression), this will involve binning or splining the age variable into several features.\n",
      " \n",
      "We're going to go for a combination of the last two options.  Because of data availability, we might consider building two separate models:\n",
      " 1. One for use on months up to 72, with age as an input splined along <6 months, 6 months-2 years, and years 3-6.\n",
      " 2. One for use on months after 72: Use the average value of the above model for months 60+.\n",
      "\n",
      "We'll quickly sketch how to build the model in 1 below.\n",
      "\n",
      "**Remark**: In practice, one might do something more complicated -- e.g., model a loan's state as (say) a Markov system with states for good standing, different delinquency states, default, and prepayment.  (For a detailed model like this, you could skim the methodology section of the FHA actual report.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Brainstorming factors\n",
      "\n",
      "Some of the factors we might want to take into account are:\n",
      " - origination DTI: higher is worse\n",
      " - origination FICO score: lower is worse\n",
      " - LTV: >120 is very bad  (this would be if we had current LTVs, not origination?)\n",
      " - age of loan\n",
      " - past delinquency status: this is a _categorical_ variable\n",
      " - the relationship of the loan interest rate and the current (predicted) interest rate\n",
      " - the home price index in the geoegraphy of the loan\n",
      " \n",
      "We notice that many of these might have **non-linear** dependence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Cross-validation\n",
      "\n",
      "Even though we are dealing with time-based forecasting, we have an alternative to cutting along time: We can randomize originations.  We will do this.\n",
      "\n",
      "We will do leave-one-out cross validation.  Because defaults are important, but occur with low frequency, we must explicitly weight them in the learning process.  Note that I'm being lazy here and reporting the final _metric_ without taking into account the oversampling of the defaults -- one can imagine making a business argument for this actually being the right thing to do."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.linear_model\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _weighted_log_loss(truth, pred, weights):\n",
      "    tot_w = len(truth)\n",
      "    epsilon = 1e-15\n",
      "    pred = map(lambda x: max(epsilon, x), pred)\n",
      "    pred = map(lambda x: min(1-epsilon, x), pred)\n",
      "    pred = np.array(pred)\n",
      "    return -sum(truth * np.log(pred) + (1-truth) * np.log(1-pred))/tot_w\n",
      "    \n",
      "def weighted_log_loss(truth, pred, weights):\n",
      "    tot_w = sum(weights)\n",
      "    epsilon = 1e-15\n",
      "    pred = map(lambda x: max(epsilon, x), pred)\n",
      "    pred = map(lambda x: min(1-epsilon, x), pred)\n",
      "    pred = np.array(pred)\n",
      "    return -weights.dot(truth * np.log(pred) + (1-truth) * np.log(1-pred))/tot_w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Randomly split the originations into 10 groups, to use in cross validation.\n",
      "loan_group_hash = {}\n",
      "random.seed(1757)\n",
      "for x in df_orig.loan_seqn:\n",
      "    loan_group_hash[x] = int( 10 * random.random() )\n",
      "    \n",
      "        \n",
      "def cut_data_on(feats, i):\n",
      "    \"\"\"\n",
      "    Use the \"loan_group_has\" to partition the feature data into 90% training and 10% testing,\n",
      "    split by originations, using leave-one-out.\n",
      "    \n",
      "    The group i that's passed in is the testing set.\n",
      "    \"\"\"\n",
      "    test  = feats[ feats.loan_seqn.apply(lambda x: loan_group_hash[x] == i) ].drop(\"loan_seqn\", axis=1)\n",
      "    train = feats[ feats.loan_seqn.apply(lambda x: loan_group_hash[x] != i) ].drop(\"loan_seqn\", axis=1)\n",
      "    return (train, test)\n",
      "\n",
      "def train_logistic(train, test):\n",
      "     ## Train\n",
      "   # prepay_regression = sklearn.linear_model.LogisticRegression( ).fit(\n",
      "   #     X = train.drop([\"is_prepay\", \"is_default\", \"weight\"], axis=1),\n",
      "   #     y = train[\"is_prepay\"] )\n",
      "   # default_regression = sklearn.linear_model.LogisticRegression( ).fit(\n",
      "   #     X = train.drop([\"is_prepay\", \"is_default\", \"weight\"], axis=1),\n",
      "   #     y = train[\"is_default\"])\n",
      "    \n",
      "    \n",
      "    \n",
      "    prepay_regression = sklearn.linear_model.SGDClassifier( loss='log', random_state = 1758, n_iter=15 ).fit(\n",
      "        X = train.drop([\"is_prepay\", \"is_default\", \"weight\"], axis=1),\n",
      "        y = train[\"is_prepay\"],\n",
      "        sample_weight = train[\"weight\"])\n",
      "    default_regression = sklearn.linear_model.SGDClassifier( loss='log', random_state = 1758, n_iter=15 ).fit(\n",
      "        X = train.drop([\"is_prepay\", \"is_default\", \"weight\"], axis=1),\n",
      "        y = train[\"is_default\"],\n",
      "        sample_weight = train[\"weight\"])\n",
      "    return (prepay_regression, default_regression)\n",
      "\n",
      "def test_on(train, test, p):\n",
      "    \"\"\"\n",
      "    Do the train - predict - report metric step for given training and testing features.\n",
      "    \"\"\"\n",
      "    (prepay_regression, default_regression) = p\n",
      "   \n",
      "    ## Predict\n",
      "    predicted_prepay, predicted_default = ( \n",
      "            map(lambda x: x[1], \n",
      "                prepay_regression.predict_proba( test.drop([\"is_prepay\", \"is_default\", \"weight\"], axis=1) ) ),\n",
      "            map(lambda x: x[1], \n",
      "                default_regression.predict_proba( test.drop([\"is_prepay\", \"is_default\", \"weight\"], axis=1) ) ) \n",
      "    )\n",
      "    \n",
      "      #  sklearn.metrics.log_loss( y_true = test[\"is_prepay\"].values, \n",
      "      #                           y_pred = map(lambda x: (1-x, x), predicted_prepay)),\n",
      "      #  sklearn.metrics.log_loss( y_true = test[\"is_default\"].values, \n",
      "      #                           y_pred = map(lambda x: (1-x, x), predicted_default))\n",
      "    return ( weighted_log_loss(test.is_prepay.values,  predicted_prepay, test.weight.values),\n",
      "             weighted_log_loss(test.is_default.values,  predicted_default, test.weight.values)\n",
      "    )\n",
      "\n",
      "def mean(l):\n",
      "    return sum(l)/len(l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Benchmark Model: Use the historical average prepay / default curves (up to year ~ 5, then constant)\n",
      "import scipy.stats.mstats\n",
      "\n",
      "def test_on_bench(train, test):\n",
      "    \"\"\"\n",
      "    Do the train - predict - report metric step for given training and testing features.\n",
      "    \"\"\"\n",
      "    # \"Train\"\n",
      "    end_prepay = mean(prepay_by_age[(prepay_by_age.index>=58) & (prepay_by_age.index<=70)]['Total'])\n",
      "    end_default = mean(default_by_age[(default_by_age.index>=58) & (default_by_age.index<=70)]['Total'])\n",
      "    \n",
      "    ## Predict\n",
      "    predicted_prepay, predicted_default = ( \n",
      "            map(lambda age: prepay_by_age['Total'][age] if age<=70 else end_prepay, test.loan_age),            \n",
      "            map(lambda age: default_by_age['Total'][age] if age<=70 else end_default, test.loan_age)\n",
      "    )\n",
      "\n",
      "    return ( \n",
      "        weighted_log_loss( test[\"is_prepay\"].values, \n",
      "                                predicted_prepay,\n",
      "                                test[\"weight\"].values ),\n",
      "        weighted_log_loss(  test[\"is_default\"].values, \n",
      "                                predicted_default,\n",
      "                                test[\"weight\"].values)\n",
      "    )\n",
      "\n",
      "def generate_features(perf):\n",
      "    return perf[[ \"loan_age\", \"loan_seqn\", \"is_prepay\", \"is_default\", \"weight\" ]]\n",
      "\n",
      "feats = generate_features(df_perf)\n",
      "metrics = [test_on_bench(*cut_data_on(feats, i)) for i in range(0,10)]\n",
      "print \"Log-loss on prepays:\", mean([m[0] for m in metrics])\n",
      "print \"Log-loss on defaults:\", mean([m[1] for m in metrics])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### A simple 'naive' model -- not taking into account non-linearity.\n",
      "## Generate Features\n",
      "def generate_features(perf):\n",
      "    feats = perf[[\"loan_seqn\", \"credit_score\", \"dti\", \"ltv\", \"loan_age\", \"past_delinq\", \"is_prepay\", \"is_default\", \"weight\" ]]\n",
      "    feats.dropna(how='any', inplace=True) \n",
      "    return feats\n",
      "\n",
      "feats = generate_features(df_perf)\n",
      "def go(train, test):\n",
      "    return test_on(train, test, train_logistic(train, test))\n",
      "metrics = [go(*cut_data_on(feats, i)) for i in range(0,10)]\n",
      "print \"Log-loss on prepays:\", mean([m[0] for m in metrics])\n",
      "print \"Log-loss on defaults:\", mean([m[1] for m in metrics])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## An exampe of trying to 'bin' a variable (credit_score, ltv), and linearly spline another (age)\n",
      "##\n",
      "## I didn't try to optimize this very well -- and for defaults it does worse than the above!\n",
      "## Nevertheless, it was worth a shot!\n",
      "\n",
      "def generate_features(perf):\n",
      "    feats = perf[[\"loan_seqn\", \"is_prepay\", \"is_default\", \"dti\", \"weight\"]]\n",
      "\n",
      "    # We believe these originations are somehow exceptionally bad.\n",
      "    feats[\"fudge_origination\"] = 1 * (2005 <= perf.year) * (perf.year <= 2007)\n",
      "\n",
      "    # Bin the LTV\n",
      "    feats[\"high_ltv\"] = 1 * (perf.ltv > 80)\n",
      "    feats[\"low_ltv\"] = 1 * (perf.ltv < 70)\n",
      "\n",
      "    # Bin the FICO score\n",
      "    feats[\"fico4\"] = 1 * (perf.credit_score > 800)\n",
      "    feats[\"fico3\"] = 1 * (perf.credit_score > 750)\n",
      "    feats[\"fico2\"] = 1 * (perf.credit_score > 700)\n",
      "    feats[\"fico1\"] = 1 * (perf.credit_score > 650)\n",
      "\n",
      "    # Bin / linearly spline (in the middle) the age\n",
      "    feats[\"1_age<=6\"] = 1 * (perf.loan_age<=6)\n",
      "    feats[\"1_6<age<=24\"] = 1 * (6<perf.loan_age) * (perf.loan_age<=24)\n",
      "    feats[\"6<age<=24\"] = feats[\"1_6<age<=24\"] * perf.loan_age\n",
      "    feats[\"1_24<age<=72\"] = 1 * (24<perf.loan_age)* (perf.loan_age<=72)\n",
      "    feats[\"24<age<=72\"] = feats[\"1_24<age<=72\"] * perf.loan_age\n",
      "    feats[\"1_age>72\"] = 1 * (perf.loan_age>72)\n",
      "\n",
      "    feats.dropna(how='any', inplace=True)\n",
      "    return feats\n",
      " \n",
      "feats = generate_features(df_perf)\n",
      "def go(train, test):\n",
      "    return test_on(train, test, train_logistic(train, test))\n",
      "metrics = [go(*cut_data_on(feats, i)) for i in range(0,10)]\n",
      "print \"Log-loss on prepays:\", mean([m[0] for m in metrics])\n",
      "print \"Log-loss on defaults:\", mean([m[1] for m in metrics])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Exercises:\n",
      "\n",
      "1. In training and testing, we might expect to hit two unfortunate effects: The origination cohorts ~2005-2007, and the economic conditions in calendar years 2007-2008, can be rightfully viewed as __atypical__.  How might you take these into account?\n",
      "\n",
      "2. Add in interest rates (http://www.quandl.com/FMAC/FIX30YR-Primary-Mortgage-Market-Survey-30-Year-Fixed-Rate) as a factor in the prepay model.  Do you see any possible problems with this methodology from the viewpoint of proper back-testing / cross-validation?\n",
      "\n",
      "3. In the above, we were measuring the log-loss over our test data.  What might be a better business metric?\n",
      "\n",
      "4. Why do you think the simpler (non-binned, fewer inputs) model did better on defaults but not on prepays?\n",
      "\n",
      "5. Describe how we are doing the sampling of originations.  Why are we doing it this way?\n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Hints to Exercises\n",
      "\n",
      "1. For the originations, one can either not train/test on them -- or add a \"fudge factor\" indicator variable for them (see the commented out source above).  For the economic conditions, one could try to avoid having those years unduly effect training or testing: e.g., use only cohorts that occur after or that have reached ~5-6 years of age before then.\n",
      " \n",
      "2. The potential issue is as follows: When running the model in the future, we will be using some source of __predicted__ future interest rates (e.g., pull it from Moody's) rather than the unknown __actual__ values.  \n",
      "\n",
      "  If this prediction has systemic sources of error as compared to reality, our cross-validation is not valid.  If we're willing to trust (or take as a black box) interest rate predictions, this is fine -- otherwise it would be better to use old __predicted__ values (e.g., pull Moody's forecasts from 2009 and test on times >= 2009 using those interest rates).\n",
      "\n",
      "3. The difference between expected and observed payouts on the loans.  Even better, the net present value (NPV) of this quantity to a fixed prediction time -- e.g., pick 2009 as the \"current time\" and predict the NPV of future loan cashflows through 2013, and compare it to the actual NPV.  This has the effect of weighting the importance of samples by remaining UPB -- this *also* has the effect of combining the impact of prepay and default predictions with an appropriate weighting.\n",
      "\n",
      "4. It could be an issue of data availability: even with the oversampling, our data sample does not have all that many defaults in it and the more complicated model tries to cut it up more.  Another possibility is the following: if we look at the curve of average defaults by loan_age, it actually does look fairly linear.\n",
      "\n",
      "5. We are over-sampling those loans which are known to lead to default.  (We are also cutting by state, for convenience..)  This is because defaults are rare, ~3.5% of loans apparently, s that this is necessary to ensure that we see enough test cases to learn from."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}