{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Prediction problem with the Yelp dataset.\n",
      "Our objective is to do machine-learning on venue review data provided by Yelp.  There are two problems that we shall try to solve.\n",
      "\n",
      "1. How well can we predict the rating of a new venue?  That is, given only basic venue attributes (such as where it is, the kind of cuisine) can we say whether or not Yelpers will tend to like it?\n",
      "\n",
      "1. An interesting NLP exercise is to see if we can predict the review's score based on the text.\n",
      "\n",
      "### Download the data\n",
      "First, go to `projects/yelp-project` and type `make` to run the Makefile.  If this doesn't work, you can download the data from [Yelp's website](http://www.yelp.com/dataset_challenge) but be sure to download all the data to the `data/` directory."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Loading and splitting data\n",
      "\n",
      "**Exercise:** Write a module that loads the business data (i.e. ones that have `{'type': 'business'}` in their definition).  The file format is a list of json blobs separated by newlines i.e. a list of inputs for the json encoder.  *Hints:* \n",
      "1. `gzip.open` ([docs](https://docs.python.org/2/library/gzip.html)) has the same interface as `open` but is for `.gz` files.\n",
      "1. `simplejson` ([docs](http://simplejson.readthedocs.org/en/latest/)) has the same interface as `json` but is *substantially* faster.  \n",
      "Note: You may need to catch errors ([docs](https://wiki.python.org/moin/HandlingExceptions)) in loading the json blobs.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Setup cross-validation:\n",
      "\n",
      "1. `cross_validation.train_test_split` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)) will split the data for you.  Don't forget that the data *might* be ordered in a systematic (i.e. pathological) way so you'll want to randomize the order first.\n",
      "\n",
      "**Exercise**: Write a function \n",
      "```python\n",
      "def evaluate(estimator, X, y):\n",
      "    return # trained model and score\n",
      "```\n",
      "which splits the data between testing and validation, trains a model, and then scores it on the test data.  This will be your testing harness.  What metric would you use?  Should this be a classification or regression problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Building models in sklearn:\n",
      "\n",
      "All estimators (e.g. linear regression, kmeans, etc ...) support `fit` and `predict` methods.  In fact, you can build your own by inheriting from classes in `sklearn.base` by using this template:\n",
      "``` python\n",
      "class Estimator(base.BaseEstimator, base.RegressorMixin):\n",
      "  def __init__(self, ...):\n",
      "   # initialization code\n",
      "\n",
      "  def fit(self, X, y):\n",
      "    # fit the model ...\n",
      "    return self\n",
      "\n",
      "  def predict(self, X):\n",
      "    return # prediction\n",
      "```\n",
      "The intended usage is:\n",
      "``` python\n",
      "estimator = Estimator(...)  # initialize\n",
      "estimator.fit(X_train, y_train)  # fit data\n",
      "y_pred = estimator.predict(X_test)  # predict answer\n",
      "estimator.score(X_test, y_test)  # evaluate performance\n",
      "```\n",
      "The regressor provides an implementation of `.score`.  Conforming to this convention has the benefit that many tools (e.g. cross-validation, grid search) rely on this interface so you can use your new estimators with the existing `sklearn` infrastructure.\n",
      "\n",
      "For example `grid_search.GridSearchCV` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html)) actually takes an estimator and some hyperparameters, and returns another estimator.  Upon fitting, it fits the best model (based on the inputted hyperparameters) and uses that for prediction.\n",
      "\n",
      "Of course, we sometimes need to transform the data before we can do machine-learning.  These are called Transformations.\n",
      "``` python\n",
      "class Transformer(base.BaseEstimator, base.TransformerMixin):\n",
      "  def __init__(self, ...):\n",
      "   # initialization code\n",
      "\n",
      "  def fit(self, X, y=None):\n",
      "    # fit the transformation ...\n",
      "    return self\n",
      "\n",
      "  def transform(self, X):\n",
      "    return # transformation\n",
      "```\n",
      "When combined with our previous `estimator`, the intended usage is \n",
      "``` python\n",
      "transformer = Transformer(...)  # initialize\n",
      "X_trans_train = transformer.fit_transform(X_train)  # fit / transform data\n",
      "estimator.fit(X_trans_train, y_test)  # fit new model\n",
      "X_trans_test = transformer.transform(X_test)  # transform data\n",
      "estimator.score(X_trans_test, y_test)  # fit new model\n",
      "```\n",
      "Here, `.fit_transform` is implemented based on the `.fit` and `.transform` methods in `base.TransformerMixin` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html)).  Especially for transformers, `.fit` is often empty and only `.transform` actually does something.\n",
      "\n",
      "The reason we use transformers is that we can chain them together with pipelines.  For example, this \n",
      "``` python\n",
      "new_model = pipeline.Pipeline([\n",
      "    ('trans', Transformer(...)),\n",
      "    ('est', Estimator(...))\n",
      "  ])\n",
      "new_model.fit(X_train, y_train)\n",
      "new_model.score(X_test, y_test)\n",
      "```\n",
      "would replace all the fitting and scoring code above.  That is, the pipeline is itself is an estimator (and implements a `.fit` and `.predict` methods).  Note that a pipeline can have multiple transformers but only one estimator.\n",
      "\n",
      "\n",
      "### Organization of project\n",
      "We're going to build a project this way:\n",
      "\n",
      "```\n",
      "data/\n",
      "src/__init__.py  # required by python\n",
      "src/loader.py  # code to load the file\n",
      "src/estimators.py  # estimators\n",
      "src/transformers.py  # transformers\n",
      "src/analysis.py  # file that actually runs the analysis\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## City based model \n",
      "\n",
      "The venues belong to different cities.  You can image that the ratings in some cities are probably higher than others and use this as an estimator.\n",
      "\n",
      "**Exercise**: Build an estimator that uses `groupby` and `mean` to compute the average rating in that city.  Use this as a predictor."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Location-based model\n",
      "\n",
      "You can imagine that a city-based model might not be sufficiently fine-grained.  For example, we know that some neighborhoods are trendier than others.  We might consider a K Nearest Neighbors or Random Forest based on the latitude longitude as a way to understand neighborhood dynamics.\n",
      "\n",
      "**Exercise**: You should implement a generic `ColumnSelectTransformer` that is passed which columns to select in the transformer and use a non-linear model like `neighbors.KNeighborsRegressor` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)) or `ensemble.RandomForestRegressor` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)) as the estimator (why would you choose a non-linear model?).  Bonus points if you wrap the estimator in `grid_search.GridSearchCV` and use cross-validation to determine the optimal value of the parameters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Categories\n",
      "\n",
      "Venues have categories with varying degrees of specificity, e.g.\n",
      "```python\n",
      "[Doctors, Health & Medical]\n",
      "[Restaurants]\n",
      "[American (Traditional), Restaurants]\n",
      "```\n",
      "With a large sparse feature set like this, we often use a cross-validated regularized linear model.\n",
      "**Exercise:**\n",
      "\n",
      "1. Build a custom transformer that massages the data so that it can be fed into `feature_extraction.DictVectorizer` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)), which in turn generates a large matrix gotten by One-Hot-Encoding.  Feed this into a Linear Regression (and cross validate it!).  Can you beat this with another type of non-linear estimator?  \n",
      "\n",
      "1. Some categoreis (e.g. Restaurants) are not very speicifc.  Others (Japanese sushi) are much more so.  How can we account for this in our model (*Hint:* look at TF-IDF)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Attributes model\n",
      "\n",
      "Venues have (potentially nested) attributes.  For example,\n",
      "``` python\n",
      "{ 'Attire': 'casual',\n",
      "  'Accepts Credit Cards': True,\n",
      "  'Ambience': {'casual': False, 'classy': False }}\n",
      "```\n",
      "Categorical data like this should often be transformed by a One Hot Encoding.  For example, we might flatten the above into something like this:\n",
      "``` python\n",
      "{ 'Attire_casual' : 1,\n",
      "  'Accepts Credit Cards': 1,\n",
      "  'Ambience_casual': 0,\n",
      "  'Ambience_classy': 0 }\n",
      "```\n",
      "**Exercise:** Build a custom transformer that flattens attributes and feed this into `DictVectorizer`.  Feed it into a (cross-validated) linear model (or something else!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Combining models\n",
      "\n",
      "So far we have only built models based on individual features.  We could obviously combine them.  One (highly recommended) way to do this is through a [feature union](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html).  \n",
      "\n",
      "**Exercise:** Combine all the above models using a feature union.  Notice that a feature union takes transformers, not models as arguements.  The way around this is to build a transformer\n",
      "\n",
      "```class ModelTransformer```\n",
      "\n",
      "that outputs the prediction in the transform method, thus turning the model into a transformer.  Use a cross-validated linear regression (or some other algorithm) to weight these signals."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2014 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}