{
 "metadata": {
  "name": "",
  "signature": "sha256:7e1a3555332bcf5c505be52cb426d55c3d1425a665e6c8730c8d0383f32d4a79"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Hadoop\n",
      "\n",
      "**Hadoop** is a distributed file system for storing data on a *distributed disk* (a collective hard disks of multiple computers in a cluster).  Hadoop commands resemble Unix commands so you should be familiar with them already.\n",
      "\n",
      "Let's get started with Hadoop.  It's already installed on your Digital Ocean box.  First, here's some stuff to test out Hadoop and mapreduce.  If these commands fail, read the notes on restarting Hadoop at the end of this document.\n",
      "\n",
      "``` bash\n",
      "# create folder on hadoop\n",
      "hadoop fs -mkdir input\n",
      "\n",
      "# move these files to hadoop\n",
      "hadoop fs -put /etc/hadoop/conf/*.xml input\n",
      "\n",
      "# check that those files are on hadoop\n",
      "hadoop fs -ls input\n",
      "\n",
      "# run this mapreduce\n",
      "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.2.0.2.0.11.0-1.jar grep input output '.*yarn.*'\n",
      "\n",
      "# note that there is an output dir\n",
      "hadoop fs -ls\n",
      "\n",
      "# it contains a _SUCCESS flag and part-r-00000\n",
      "hadoop fs -ls output\n",
      "\n",
      "# here's the contents\n",
      "hadoop fs -cat output/part-r-00000 | head\n",
      "```\n",
      " \n",
      "Notice that the above code copies some of your Hadoop configuration files `/etc/hadoop/conf/*.xml` onto HDFS (Hadoop Distributed File System) under the folder `input`.  It then runs a `grep` mapreduce looking for words that start with `dfs` and places the reuslts into the folder `output`.  You should verify its contents look something like this:\n",
      "\n",
      "    1\t      <name>yarn.scheduler.capacity.root.queues</name>\n",
      "    1\t      <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>\n",
      "    1\t      <name>yarn.scheduler.capacity.root.default.state</name>\n",
      "    1\t      <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>\n",
      "    1\t      <name>yarn.scheduler.capacity.root.default.capacity</name>\n",
      "    1\t      <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>\n",
      "    1\t      <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>\n",
      "    1\t      <name>yarn.scheduler.capacity.node-locality-delay</name>\n",
      "    1\t      <name>yarn.scheduler.capacity.maximum-applications</name>\n",
      "    1\t      <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>\n",
      "    1\t      <name>yarn.resourcemanager.hostname</name>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If these commands do not work due to a \"connection error\", you may need to restart hadoop:\n",
      "\n",
      "```bash\n",
      "sudo su root  # become the root user\n",
      "echo JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64 >> /etc/environment\n",
      "# run command-D to get out of root mode\n",
      "\n",
      "# Start HDFS\n",
      "sudo service hadoop-hdfs-datanode start\n",
      "sudo service hadoop-hdfs-namenode init\n",
      "sudo service hadoop-hdfs-namenode start\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Building a WordCount program.\n",
      "\n",
      "Mapreduce is written in Java which means that directory tree is deep and the code is loquacious.  Open up [WordCount.java](../projects/MR/src/main/java/com/thedataincubator/hadoopexamples/WordCount.java).  Yes - word count is going to be 100 lines long.  Let's begin with the mapper, which declares that is of type `Mapper <k0, v0, k1, v1>`:\n",
      "\n",
      "``` java\n",
      "public static class WordTokenizerMapper\n",
      "  extends Mapper<Object, Text, Text, IntWritable>\n",
      "```\n",
      "      \n",
      "The meat of the code is these 5 lines:\n",
      "\n",
      "``` java\n",
      "IntWritable one = new IntWritable(1);\n",
      "Text word = new Text();\n",
      "\n",
      "StringTokenizer tokenizer \n",
      "    = new StringTokenizer(value.toString(), \" \\t\\n\\r\\f,.:;?![]'\\\"\");\n",
      "String s = tokenizer.nextToken().toLowerCase().trim();\n",
      "word.set(s);\n",
      "context.write(word, one);\n",
      "```\n",
      "\n",
      "Because mapreduce has to write everything to disk, it needs to use wrappers (called `Writable`s) for basic types which can *serialize* basic datatypes.  `IntWritable` is a wrapper for `int` and `Text` is a wrapper for `String`.  `IntWritable` has a `set` method which sets the value being wrapped.  The string is split by `StringTokenizer` into words and then each word is outputted with the number `one` (wrapped as an `IntWritable`).\n",
      "\n",
      "The reducer has to declare that it is of type `Reduer <k1, v1, k2, v2>` (and note that `k1` and `v1` in the mapper and reducer match):\n",
      "\n",
      "``` java\n",
      "public static class WordOccurrenceReducer\n",
      "  extends Reducer<Text, IntWritable, Text, IntWritable>\n",
      "```\n",
      "      \n",
      "Its logic is expressed (rather garrulously) in a few lines:\n",
      "      \n",
      "``` java\n",
      "private IntWritable occurrencesOfWord = new IntWritable();\n",
      "\n",
      "int sum = 0;\n",
      "for (IntWritable val : values) {\n",
      "    sum += val.get();\n",
      "}\n",
      "occurrencesOfWord.set(sum);\n",
      "context.write(key, occurrencesOfWord);\n",
      "```\n",
      "    \n",
      "Finally, there's a `main` function which is the entry-point of a java program and where the mapper, `FileInputFormat`, `FileOutputFormat` etc ... are specified.  You'll notice that Java is a little slow at learning and you'll have to specify the same things in multiple places.\n",
      "\n",
      "``` java\n",
      "public static void main(String[] args)\n",
      "  throws Exception\n",
      "```\n",
      "      \n",
      "**Exercise:**\n",
      "1. Add a `combiner` to speed things up.  You can reach about the `combiner` in the [Apache Tutorial](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html) (just look for the word combiner).\n",
      "1. You should have all the basic ingredients to write a Hadoop version of Unix's wordcount.  That is, accept a file and write 3 outputs, which correspond to the number of words, the number of characters in those words, and the number of lines.\n",
      "1. [SpendingByUser.java](../projects/MR/src/main/java/com/thedataincubator/hadoopexamples/SpendingByUser.java) is a very similar program that reads in a tab-delineated file [transactions.tsv](../projects/MR/data/transactions.tsv) representing transactions at a hypothetical store.  The mapreduce outputs a pair of userid and total spending amount, e.g. :\n",
      "    \n",
      "        u1, 150\n",
      "    \n",
      " Modify it to output the average purchase amount and standard deviation of the purchase amounts:\n",
      "  \n",
      "        u1 mean, 50\n",
      "        u1 std, 23\n",
      "  \n",
      " How would you use a combiner in this case?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A note about how to run mapreduce\n",
      "\n",
      "Download some sample texts from Project Gutenberg and move them to HDFS.  We are going to use them to count our datafile.\n",
      "\n",
      "``` bash\n",
      "# copy data and check that it made it\n",
      "cd ~/datacourse\n",
      "hadoop fs -copyFromLocal small_data/gutenberg gutenberg\n",
      "hadoop fs -ls gutenberg\n",
      "```\n",
      "    \n",
      "This example is based on [this tutorial](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/).  First, you should make sure you can find the [pom.xml](../projects/MR/pom.xml).  This is an `xml` file that serves as a `makefile` for maven, a Java compile tool.  `cd` to the containing folder on Digital Ocean and invoke the maven compile command\n",
      "\n",
      "``` bash\n",
      " cd projects/MR/\n",
      " mvn clean install\n",
      " ```\n",
      "\n",
      "If this is your first time running things, maven will download a lot of stuff (this could take some time).  Eventually, you should see a build success message like this\n",
      "\n",
      "    [INFO] ------------------------------------------------------------------------\n",
      "    [INFO] BUILD SUCCESS\n",
      "    [INFO] ------------------------------------------------------------------------\n",
      "    [INFO] Total time: 2.288 s\n",
      "    [INFO] Finished at: 2014-05-25T19:40:08-05:00\n",
      "    [INFO] Final Memory: 16M/81M\n",
      "    [INFO] ------------------------------------------------------------------------\n",
      "\n",
      "If instead of `BUILD SUCCESS` you see `BUILD FAILURE`, look immediately above it for the build errors, fix them, and try again.  When you see `BUILD SUCCESS`, you should find `hadoop-examples-1.0.jar` under `target`:\n",
      "\n",
      "``` bash\n",
      "ls target/hadoop-examples-1.0.jar\n",
      "```\n",
      "\n",
      "Confirm that the file is there.  Once it is, you can run \n",
      "\n",
      "``` bash\n",
      "# run mapreduce\n",
      "hadoop jar target/hadoop-examples-1.0.jar com.thedataincubator.hadoopexamples.WordCount gutenberg gutenberg-output\n",
      "\n",
      "# retrieve the data\n",
      "hadoop fs -cat gutenberg-output/part-r-00000 | more\n",
      "```\n",
      " \n",
      "You should see something like \n",
      "\n",
      "    14/05/25 19:56:29 INFO input.FileInputFormat: Total input paths to process : 3\n",
      "    14/05/25 19:56:30 INFO mapred.JobClient: Running job: job_201405231953_0030\n",
      "    14/05/25 19:56:31 INFO mapred.JobClient:  map 0% reduce 0%\n",
      "    14/05/25 19:56:42 INFO mapred.JobClient:  map 67% reduce 0%\n",
      "    14/05/25 19:56:48 INFO mapred.JobClient:  map 100% reduce 0%\n",
      "    14/05/25 19:56:51 INFO mapred.JobClient:  map 100% reduce 100%\n",
      "    14/05/25 19:56:53 INFO mapred.JobClient: Job complete: job_201405231953_0030\n",
      "\n",
      "Followed by a bunch of stats about Hadoop's performance.  This output is periodically give you updates on the progress fo the map and reduce phase.  *NB*: `com.thedataincubator.hadoopexamples.WordCount` corresponds to two places:\n",
      "\n",
      "1. The first line in `WordCount.java` is `package com.thedataincubator.hadoopexamples;`  This is necessary for invoking the command line when running hadoop: it tells the JVM where to find the package at runtime.\n",
      "1. The file is located in `src/main/java/com/thedataincubator/hadoopexamples/WordCount.java`.  This is done for Maven: it expects to find the files there when it's looking for files to compile.\n",
      "\n",
      "As you add and modify files for this lesson, keep these two invariants constant.  Otherwise, you'll spend your time fighitng either Maven or the JVM."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Joining data in mapreduce\n",
      "\n",
      "We are running a store and we have information about our users in [`users.tsv`](../projects/MR/data/users.tsv) by user id, email, language, country:\n",
      "\n",
      "    u1\tbob@exmaple.com\tEN\tUS\n",
      "    u2\tjon@exmaple.com\tEN\tGB\n",
      "    u3\tsally@exmaple.com\tFR\tFR\n",
      "    u4\tkatie@exmaple.com\tFR\tFR\n",
      "    \n",
      "We also have their transactions in [`transactions.tsv`](../projects/MR/data/transactions.tsv) by transaction id, product id, user id, and amount:\n",
      "    \n",
      "    t1\tp1\tu1\t100\n",
      "    t2\tp4\tu2\t150\n",
      "    t3\tp3\tu1\t200\n",
      "    t4\tp4\tu4\t50\n",
      "    t5\tp1\tu3\t100\n",
      "    t6\tp2\tu1\t100\n",
      "    \n",
      "What if we wanted to join these two datasets together by userid to get the amount spent in each country?  To do this in mapreduce, we have to first join the data.\n",
      "\n",
      "Joining data in mapreduce requires that we use two mappers, one for `users.tsv` and the other for `transactions.tsv`.  The trick is that the output of both mappers must be the same type (i.e. `(k1, v1)` have to be the same for both mappers) so that they can be fed to a common reducer which performs the join.  What types will we use?  The key `k1` is clear: we're joining these two tables on user id.  The value is less clear.  We want `users.tsv` to output the country of the user, but we want `transactions.tsv` to output the amount that user spent on each transaction.  We can do both by creating a special `Writable` that is the union of the two, which is done in [`AmountOrCountry.java`](../projects/MR/src/main/java/com/thedataincubator/hadoopexamples/AmountOrCountry.java):\n",
      "\n",
      "``` java\n",
      "public class AmountOrCountry\n",
      "implements Writable\n",
      "{\n",
      "  public AmountOrCountry(){}\n",
      "\n",
      "  public Text type = new Text();\n",
      "  public IntWritable amount = new IntWritable();\n",
      "  public Text country = new Text();\n",
      "\n",
      "  public void setAmount(int amount) {\n",
      "    type.set(\"amount\");\n",
      "    this.amount = new IntWritable(amount);\n",
      "  }\n",
      "\n",
      "  public void setCountry(String country) {\n",
      "    type.set(\"country\");\n",
      "    this.country = new Text(country);\n",
      "  }\n",
      "\n",
      "  public void write(DataOutput out) throws IOException {\n",
      "    type.write(out);\n",
      "    amount.write(out);\n",
      "    country.write(out);\n",
      "  }\n",
      "\n",
      "  public void readFields(DataInput in) throws IOException {\n",
      "    type.readFields(in);\n",
      "    amount.readFields(in);\n",
      "    country.readFields(in);\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "The class has 3 fields: `type`, `amount`, and `country`.  If `type == \"amount\"`, then the `amount` field is used and the `country` field is ignored, and vice-versa if `type == \"country\"`.  All `Writable` classes have to implement `write` and `readFields` which write the data out to disk and read them back in.  They do this by calling the methods on the corresopnding values `type`, `amount`, and `country`.  The only trick is to make sure we write the fields out in the same order in which we read them back in.\n",
      "\n",
      "Then we can see that the output type (`k1, v1`) of `CountryMapper`\n",
      "\n",
      "``` java\n",
      "public static class CountryMapper\n",
      "  extends Mapper<LongWritable, Text, Text, AmountOrCountry>\n",
      "```\n",
      "\n",
      "matches that of `AmountMapper`\n",
      "\n",
      "``` java\n",
      "public static class AmountMapper\n",
      "  extends Mapper<LongWritable, Text, Text, AmountOrCountry>\n",
      "```\n",
      "\n",
      "Finally, notice that in `CountryMapper` we use `setCountry` method\n",
      "\n",
      "``` java\n",
      "public static AmountOrCountry outValue = new AmountOrCountry();\n",
      "\n",
      "outValue.setCountry(country);\n",
      "context.write(outKey, outValue);\n",
      "```\n",
      "\n",
      "while in `AmountMapper` we use the `setAmount` method\n",
      "\n",
      "``` java\n",
      "public static AmountOrCountry outValue = new AmountOrCountry();\n",
      "\n",
      "outValue.setAmount(amount);\n",
      "context.write(outKey, outValue);\n",
      "```\n",
      "\n",
      "The reducer is a little trickier.  We keep track of the running sum (as with `WordCount`) but for each record, we have to check whether we should use the `country` field or the `amount` field:\n",
      "\n",
      "``` java\n",
      "int sum = 0;\n",
      "for (AmountOrCountry value: values) {\n",
      "    if (value.type.toString().equals(\"amount\")) {\n",
      "        sum += value.amount.get();\n",
      "    } else { // \"country\"\n",
      "        country.set(value.country.toString());\n",
      "    }\n",
      "}\n",
      "sumWritable.set(sum);\n",
      "context.write(country, sumWritable);\n",
      "```\n",
      "\n",
      "Without using the Secondary Sort (for a great explanation, checkout [Matthew Rathbone's blog entry](http://blog.matthewrathbone.com/2013/02/09/real-world-hadoop-implementing-a-left-outer-join-in-hadoop-map-reduce.html)), there's no way to gaurantee the order of the records passed to reduce.  The code is written so that the \"country\" entry could happen at any point.\n",
      "\n",
      "**Question**: what happens if multiple country values are passed for a user entry?  More likely, what happens if none is?  What does this mean about the user database?\n",
      "\n",
      "**Exercises**:\n",
      "1. Let's test it out.  Move the `users.tsv` and `transactions.tsv` files onto HDFS.  Checkout the `main` function to see how to pass parameters to the hadoop job and test it out!\n",
      "1. If you look at the output, you'll see we aren't quite done.  There are still multiple entries per country.  We still need to aggregate the results by country.  Write a second mapreduce that aggregates by country (hint, use an identity mapper).\n",
      "1. For a very comprehensive list of questions and answers, check out [this site](http://meri-stuff.blogspot.com/2011/10/mapreduce-questions-and-answers.html)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Alternatives:\n",
      "\n",
      "As you can see, Java Mapreduce turns five lines of SQL into hundreds of lines of java code.  There are a few well-regarded solutions that try to get around this:\n",
      "1. **Hadoop Streaming**: This creates a mapreduce by specifying a map and reduce script.  Map and reduce scripts take are take input via stdin and output data via stdout.  The scripts can be anything (bash, python, etc ...).  The key and value are separated by the first tab value.  While its very quick to get setup and easy to learn, you have to do a lot of jenky serialization.  It also doesn't have real support for joins (instead of having two mappers, one for each input, you need to have one mapper that handles both inputs).  ([Read more](https://hadoop.apache.org/docs/r1.2.1/streaming.html)).  Yelp! built ([mrjob](https://github.com/Yelp/mrjob) as a python wrapper ontop of hadoop streaming).\n",
      "1. **Hive**: Hive is a SQL interface to Mapreduce.  The user writes SQL-like queries and they are translated into mapreduce under the hood.  It only supports a subset of the SQL language.  ([Read more](https://cwiki.apache.org/confluence/display/Hive/Tutorial)).\n",
      "1. **Pig**: Similar to Hive in that one writes in a relatively declarative language (called Pig Latin) but is more imperative than Hive.  ([Read more](https://pig.apache.org/docs/r0.7.0/piglatin_ref1.html)).\n",
      "1. **Scalding**: Scalding is a highly declarative Scala DSL (domain specific language - think of DSLs as really fancy libraries with their own custom language / syntax).  You treat data as if they were lists in Scala.  For example, here's word count:\n",
      "\n",
      "        package com.twitter.scalding.examples\n",
      "\n",
      "        import com.twitter.scalding._\n",
      "\n",
      "        class WordCountJob(args : Args) extends Job(args) {\n",
      "          TextLine( args(\"input\") )\n",
      "            .flatMap('line -> 'word) { line : String => tokenize(line) }\n",
      "            .groupBy('word) { _.size }\n",
      "            .write( Tsv( args(\"output\") ) )\n",
      "\n",
      "          // Split a piece of text into individual words.\n",
      "          def tokenize(text : String) : Array[String] = {\n",
      "            // Lowercase each word and remove punctuation.\n",
      "            text.toLowerCase.replaceAll(\"[^a-zA-Z0-9\\\\s]\", \"\").split(\"\\\\s+\")\n",
      "          }\n",
      "        }\n",
      "\n",
      "    It's well documented and has a lot of support since it was developed by Twitter.  It's easy to get started with ([Read more](https://github.com/twitter/scalding)).  Scoobi is very similar but much less well maintained ([Read more](https://github.com/NICTA/scoobi)).  Spark is similar and developed at Berkeley and promises to cache data processing into ram ([Read more](https://spark.apache.org/))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Note on restarting Hadoop\n",
      "If your Hadoop commands do not work, try runnig these commands\n",
      "\n",
      "```bash\n",
      "sudo su root  # become the root user\n",
      "echo JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64 >> /etc/environment\n",
      "# run command-D to get out of root mode\n",
      "\n",
      "# Start HDFS\n",
      "sudo service hadoop-hdfs-datanode start\n",
      "sudo service hadoop-hdfs-namenode init\n",
      "sudo service hadoop-hdfs-namenode start\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2014 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}