{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Online learning\n",
      "This is the fancy word for machine learning when you feed your data in little-by-little.  This could be just because you cannot hold all of the data in memory / disk / your cluster at once, or it could be because you are getting new data over time and feeding it into an existing model.\n",
      "\n",
      "## Out-of-core learning\n",
      "This is the fancy word for machine learning when your data doesn't all fit into memory / fit on one computer.  Online learning algorithms give, in particular, out-of-core learning algorithms.  \n",
      "\n",
      "(You could imagine differences in desired behavior -- e.g., an online learner might depend on the order in which you feed in the data, so if you do not want this you might shuffle the data first.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%install_ext https://bitbucket.org/preygel/ipython-diags/raw/default/diagmagic.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reload_ext diagmagic\n",
      "%setdiagsvg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Structure for online learning\n",
      "\n",
      "In online learning, you often want a pipeline operating on instances in a __stream__.  The point of streaming (i.e., what does streaming actually mean) is to use constant memory by keeping around only a bounded amount of history / state."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%blockdiag\n",
      "{\n",
      "  // node shapes for flowcharts\n",
      "  I [shape = roundedbox, label=\"1. Ingest data\" ];\n",
      "  E [shape = roundedbox, label=\"2. Extract features\" ];\n",
      "  U [shape = roundedbox, label=\"3. Update learner\" ];\n",
      "  I -> E -> U\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The three steps are:\n",
      "1. **Ingest data (in a _stream_ or in _chunks_)**: This means do not store everything.  In Python this usually means something like using a generator / a library that does this automatically.\n",
      "2. **Extract features (in a _stateless_ way)**: Being stateless means that your feature extractor should not store (much, or a growing amount of) information.  \n",
      "\n",
      "   This is most relevant in textual tasks, or when you have categorical features whose set of possible values is not known ahead of time -- the standard solution in both cases is to use the [_hashing trick_](http://scikit-learn.org/dev/modules/feature_extraction.html#feature-hashing).  We already mentioned this in the context of the `HashingVectorizer` for bag-of-words, but the same can be done for other categorical variables (see the [FeatureHasher](http://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher))\n",
      "3. **Update the learner**: This means to use the features to update the learned coefficients of the model.  In scikit-learn this is the `partial_fit` method that some (but not all) learners have.  \n",
      "\n",
      "  In general there is some question of _what this means_ for any given learner: i.e., are more recent test cases given greater weight in the optimization problem being solved, and if so how much greater? \n",
      "\n",
      "  (The answer is usually yes -- they are given greater weight.  This behavior is often desired -- where it might not be is if you are using an online learner to do out-of-core learning on data that does not have a natural ordering by relevance / time, in which case a shuffling step will help.) "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Online learning (in e.g., scikit-learn)\n",
      "\n",
      "It turns out that scikit-learn already has some support for online learning strategies.  See http://scikit-learn.org/dev/modules/scaling_strategies.html#incremental-learning for a full list, but some notable examples are:\n",
      "\n",
      "1. Stochastic gradient descent methods: `SGDClassifier` and `SGDRegressor`.  These can be used to train the standard _linear models_:  _linear regressions_ for regression; and, _logistic regressions_ and _SVM_ for classification.\n",
      "\n",
      "  Some caveats: \n",
      "   - In contrast to some other linear learners, SGD methods care a lot that the data be _scaled_ \n",
      "   - There are extra (numerical) parameters that must be tuned to ensure good learning.\n",
      "\n",
      "2. Naive Bayes methods: `MultinomialNB`, `BernoulliNB`.  It's just counting, after all.\n",
      "\n",
      "3. (Unsupervised) K-Means clustering: `MiniBatchKMeans`.\n",
      "\n",
      "####For more details:\n",
      "http://scikit-learn.org/dev/modules/scaling_strategies.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Out-of-core - but not necessarily online - learning (in e.g., Spark's MLLib)\n",
      "\n",
      "There are out-of-core algorithms that do not depend on an online algorithm.  That is, you distribute the data and do some iterative training step across a cluster -- rather than feeding in the data bit by bit and (e.g.,) exponentially weighting the recent test cases more.\n",
      "\n",
      "  So for instance, while scikit-learn's SGD learners will weight recent test cases higher (and thus depends on the ordering and the  size of the batches that you feed into `partial_fit`), the logistic regression that we saw implemented in Spark last time does not. \n",
      "  \n",
      "  Similarly the algorithms in Spark's MLLib tend to be out-of-core but _not necessarily_ online.  See the documentation link below for more information, but notable entries are:\n",
      "   - SGD for linear models: (regularized) linear regressions; logistic regressions and SVM.\n",
      "   - Naive bayes\n",
      "   - Decision trees (you should look into the exact state of random forest training, though)\n",
      "   - Neural networks (you should look into exactly which variants, and how good the learning really is...)\n",
      "   - k-means clustering\n",
      "   - non-negative matrix factorizations\n",
      "   \n",
      "#### For more details:\n",
      "http://spark.apache.org/docs/latest/mllib-guide.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Other\n",
      "\n",
      "The above two sets of libraries are by no means exhaustive.  However the further you move from the \"very standard\" algorithms (note the large intersection of the above two lists) the more novelty and engineering work tends to be involved -- and it's probably most practical to hope that someone else has already done it for you.  \n",
      "\n",
      "In addition to Spark/MLLib above, there is at least the following additional place to look at for this:\n",
      "\n",
      "- [Mahout](https://mahout.apache.org/) is an Apache project that aims to provide a toolbox of \"scalable\" machine learning algorithms. It used to be based on Hadoop/MR, and now aims to move towards being based on Spark.  See\n",
      "https://mahout.apache.org/users/basics/algorithms.html for a list of implemented algorithms (some in MR, some only on a single machine, at present very little is based on Spark)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2014 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}