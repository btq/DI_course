{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## NYC Restaurant Hygene Inspections\n",
      "\n",
      "The city of New York does restaurant inspections and assigns a grade.  Inspections data the last 4 years are available [here](https://nycopendata.socrata.com/Health/Restaurant-Inspection-Results/4vkw-7nck).\n",
      "\n",
      "**Exercise** \n",
      "\n",
      "1. Download and unzip the data into the `projects/hygiene` project directory.  Remember that we are using the following file layout:\n",
      "\n",
      "    ```\n",
      "    /data  # data is in here\n",
      "    /src  # code is in here\n",
      "    /Makefile\n",
      "    ```\n",
      "    *Hint*: [wget](http://en.wikipedia.org/wiki/Wget#Using_Wget) is the linux utility for downloading files and `unzip foo.zip -d dir/` will unzip the contents of `foo.zip` to `dir/`. \n",
      "\n",
      "1. When you have downloaded the data, checkout the `*.xls` file to understand what the fields in the datafile you unzipped represent.\n",
      "\n",
      "1. **Challenge**: Document how you got your data by writing a makefile that downloads your data.  Makefile is a useful tool for setting up a chain of computations involving setting up files.  It specifies a chain of dependencies in this form\n",
      "\n",
      "    ```bash\n",
      "    output_file: input_file:\n",
      "        generate_output_file input_file  # this is a unix command\n",
      "    ```\n",
      "    Typing \n",
      "    ```bash\n",
      "    Makefile outputfile\n",
      "    ```\n",
      "    will take one of three steps:\n",
      "\n",
      "    1. if `input_file` was modified after `output_file`, it will execute the unix command `generate_output_file input_file`   \n",
      "    1. If `input_file` was modified before `output_file` nothing happens.\n",
      "    1. If `input_file` does not exist, it will look for a rule where `input_file` is on the left hand of the colon and recurisvely resolve that rule first, before executing `generate_output_file input_file`.\n",
      "\n",
      "In this way, it sepcifies a chain of *idempotent* operations to be executed.  For more information, see this [tutorial](http://mrbook.org/tutorials/make/)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Python Pandas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Loading the data\n",
      "\n",
      "The four CSV files you care to load are \n",
      "```\n",
      "Action.txt\n",
      "Cuisine.txt\n",
      "Violation.txt\n",
      "WebExtract.txt\n",
      "```\n",
      "\n",
      "**Exercise**: Load all these files (and the NYC Borough definitions) into panda dataframes.  At the end of this, each of the 5 files should be a df:\n",
      "\n",
      "``` python\n",
      "df_action\n",
      "df_cuisine\n",
      "df_violation\n",
      "df_grades  # WebExtract.txt\n",
      "df_boro  # data in *.xls\n",
      "```\n",
      "\n",
      "1. First look at `Cuisine.txt`.  This is a mapping of `CUISINECODE` (cuisine codes) to `CODEDESC` (the english description of the cuisine type).  Notice that it is really a csv file.  `pd.read_csv` does a pretty good job reading files.\n",
      "\n",
      "1. Next, notice that `data/WebExtract.txt` and ``Action.txt`` have date fields.  If you just naively read in the file, it will not recognize them and treat them as strings.  Write a customer parser \n",
      "    ```python\n",
      "    def date_parser(date_str):\n",
      "        pass\n",
      "    ``` \n",
      "and pass it into the `date_parser` kwarg of `pd.read_csv` to parse the date (see [this Stackoverflow](http://stackoverflow.com/questions/15465087/date-conversion-in-pandas-csv-reader) for more hints).  Be careful: you should be returning `None` if you are unable to parse `date_str`!\n",
      "\n",
      "1. Next, notice that `Violation.txt` is a *malformed* csv file because it doesn't escape it's strings properly and has \"extra\" commas.  Malformed data is not uncommon.  Can you think of a way to parse this in correctly (you'll need to write some custom python code for this one)?\n",
      "\n",
      "1. Finally, open up the `*.xls` file, which explains the mapping from borough codes (`BORO`) to the actual NYC Borough names (e.g. Manhattan)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scores and grades\n",
      "\n",
      "**Exercise**: Read more about how restaurant are rated [here](http://www.nyc.gov/html/doh/downloads/pdf/rii/how-we-score-grade.pdf).  According to the document, a higher score indicates a worse grade.  Plot a histogram of grades, colored by score to confirm this and confirm their numbers for the cutoff.  Do you see a sharp dropoff between score 13 and 14?  Are their any hypotheses as to why this might be the case?  *Hint*: you can use a stacked bar graph since there aren't that many possible scores once you censor for outliers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Analysis by time\n",
      "\n",
      "If you were able to parse the datetime fields correctly, you should be able to select off all rows where `'CURRENTGRADE'` is not null.\n",
      "\n",
      "`df_graded = df_grades.ix[~pd.isnull(df_grades['CURRENTGRADE'])]`\n",
      "\n",
      "**Exercise:** Plot the mean `score` as a function of year and month.  Which year (month) was the worst?  *Hints*: \n",
      "1. `dt.year` will return the year of a datetime object.  Similarly for month.\n",
      "1. You should use the [split combine aply](http://pandas.pydata.org/pandas-docs/stable/groupby.html) idiom of pandas.  In particular, you might find this syntax helpful\n",
      "\n",
      "```python\n",
      "df.groupby('col1')['col2'].mean().plot(title='col2 by col1')\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Analysis of the most recent grades\n",
      "\n",
      "Use a `groupby` to confirm that restaurants might have multiple ratings, but that they happen at different times (*Hint*: take advantage of the unique key for a restuarant).\n",
      "\n",
      "**Exercise:** \n",
      "\n",
      "1. Build a datatable consisting of only the latest grade for each restaurant.  *Hint:* `dt.argmax()` gives the index of the latest date if `dt` is a date field.\n",
      "\n",
      "1. What zipcodes have the highest final scores (i.e. worst rating)?  Notice that some zipcodes only have a few restaurants and these are not very reliable results so they should be filtered out from your analysis.\n",
      "\n",
      "1. Which borough have the highest final scores (i.e. worst rating)?  You should join in with `df_boro` to give the actual names of the boroughs.  Notice that you can use\n",
      "\n",
      "    ```python\n",
      "    df.groupby('BORO')['SCORE'].agg({'count': len, 'SCORE': np.mean})\n",
      "    ```\n",
      "to compute two aggregations on the same column 'SCORE'.\n",
      "\n",
      "1. What cuisine types have the highest final scores (i.e. worst rating?).  Note that you need to both filter for unusual cuisine types and join in with `df_cuisine` to get the Engilsh cuisine names from their cuisine codes.\n",
      "\n",
      "1. **Challenge:** Notice that the above functions repeat a lot of the same behavior.  Rewrite the above code into one function that takes different arguments to solve the above routines.\n",
      "\n",
      "1. **Challenge:** Is there a correlation between income and scores?  Downlaod data from the [Census](https://www.census.gov/epcd/www/zipstats.html) to answer this question."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Analysis of critical violations.\n",
      "\n",
      "Notice that violations are either labeled as critical or not.  Join in `df_violations` to get this.  Be careful: the violation codes appear to map to different violations depending on when they were issued!\n",
      "\n",
      "**Exercise:**\n",
      "1. Which Zipcode, Boro, or cuisine type have the largest fraction of restaurants containing at least one critical violation?  If you were able to solve the last Challenge question, then you should be able to reuse the code from above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SQL"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load data in SQL\n",
      "\n",
      "Let's try to do some things in sql.  Startup your favourite `SQL` dialect.  For `sqlite`, the command is\n",
      "``` bash\n",
      "sqlite3 data/db.sqlite\n",
      "```\n",
      "\n",
      "Load the files into your data.  In `sqlite`, the command to load `Cuisine.txt` is the formula\n",
      "``` sql\n",
      "drop table if exists cuisine;\n",
      "create table cuisine (\n",
      "\tcuisinecode integer,\n",
      "\tcuisinedesc text\n",
      ");\n",
      ".separator \",\"\n",
      ".import \"data/Cuisine.txt\" cuisine\n",
      "delete from cuisine where cuisinecode like \"%CUISINECODE%\";\n",
      "```\n",
      "\n",
      "**Exercise:** Load all 5 tables (including `insert into` statements for the boroughs)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Critical violations and average score by borough, zipcode, and cuisine\n",
      "\n",
      "**Exercise:** Pratice generating the boroughs, zipcodes, and cuisines with the most (and least) critical violations and highest (and lowest) average score.\n",
      "1. If the 'code' is not very \"human readable\", be sure to join in with another data source.\n",
      "1. Sometimes, zipcodes or cuisine types only have a few restaurants.  Be sure to filter for these."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Violations by cuisine type\n",
      "\n",
      "Which cuisines tend to have a disproportionate number of what which violations?  Answering this question isn't easy becuase you have to think carefully about normalizations.  For example, if there's a very popular cuisine category, it might just have a lot of all violations to begin with.  This suggests we should use a ratio of some kind.  Keep in mind that ratios are meaningless if the denominator is small.\n",
      "\n",
      "**Exercise:** (this is a **Challenge**).  Write a query to calculate this. For the query, you might want to checkout this [Stackoverflow post](http://stackoverflow.com/questions/972877/calculate-frequency-using-sql)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2014 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}