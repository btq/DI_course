{
 "metadata": {
  "name": "",
  "signature": "sha256:ac4689d3aacfa26983e2a544a0c5848242fb74db5f1713e7e73c8ef71f095bb3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Hadoop and MapReduce\n",
      "Sources: *Hadoop: The Definitive Guide, 4th Ed.*\n",
      "## What *is* Hadoop?\n",
      "A reliable, scalable platform for storage and analysis. Its three primary components are a resource manager, a fault tolerant, distributed filesystem, and fault-tolerant distributed processing. \n",
      "\n",
      "### Cluster computing\n",
      "**Compute node**: A single processor with main memory, cache, and local disk  \n",
      "**Computer Clusters:** Large collections of commodity hardware. Compute nodes are stored on *racks* (8-64 per rack); the nodes on a single rack are connected by a network; racks are interconnected by another level of network/switch\n",
      "- Common failure modes: loss of a single node (e.g. a node's disk crashes); the loss of an entire rack\n",
      "- Cluster computing must therefore be fault-tolerant, and files must be stored redundantly \n",
      "- Computations must be divided into tasks, such that if any one task fails to execute to completion, it can be restarted without affecting other tasks\n",
      "\n",
      "## HDFS\n",
      "*A filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware*  \n",
      "\"Streaming data access\": Write-once, read-many-times.\n",
      "- Distributed filesystem: a filesystem that manages the storage across a network of machines. A complex problem! Network of machines\n",
      "- Tolerates node failure without suffering data loss.\n",
      "- Optimized for delivering a high throughput of data, maybe at expense of latency. HBase is better for low-latency access\n",
      "\n",
      "#### Exercise: Get acquainted with HDFS\n",
      "Your Digital Ocean box already has a Hadoop 2.x version installed. This means that the DO box, which is effectively a 16 GB RAM, quad-core computer in the cloud, can act like a single node running Hadoop. Hadoop can be run as a single Java process in non-distributed mode (i.e. standalone), or in pseudo-distributed mode where each Hadoop daemon (background process) runs in a separate process. That means on the box you can create and run Hadoop jobs that access the Hadoop distributed filesystem. This is useful for debugging.\n",
      "\n",
      "At the command line of your DO box, try out a few HDFS commands:\n",
      "```bash\n",
      "$ hadoop fs -ls /\n",
      "Found 3 items\n",
      "drwxrwxrwt   - hdfs supergroup          0 2015-03-27 13:42 /tmp\n",
      "drwxr-xr-x   - hdfs supergroup          0 2015-03-25 15:35 /user\n",
      "drwxr-xr-x   - hdfs supergroup          0 2015-03-25 15:35 /var\n",
      "\n",
      "$ hadoop fs -du -h /user\n",
      "\n",
      "$ hadoop fs -mkdir gutenberg\n",
      "\n",
      "$ hadoop fs -copyFromLocal /home/vagrant/datacourse/small_data/gutenberg gutenberg\n",
      "\n",
      "$ hadoop fs -cat gutenberg/pg20417.txt | head\n",
      "```\n",
      "A full list of available commands available [here](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html).\n",
      "\n",
      "## YARN\n",
      "**YARN** (\"Yet Another Resource Negotiator\"): a cluster resource management system introduced in Hadoop 2.0 (c. February 2013) which allows any distributed program (not just MapReduce) to run on data in a Hadoop cluster.\n",
      "Critical for allowing new processing models in Hadoop, i.e. the flourishing of the Hadoop ecosystem. \n",
      "\n",
      "### The different processing patterns that work with Hadoop\n",
      "- **Interactive SQL**  \n",
      "By dispensing with MapReduce and using a distributed query engine that uses dedicated \u201calways on\u201d daemons (like Impala) or container reuse (like Hive on Tez), it\u2019s possible to achieve low-latency responses for SQL queries on Hadoop while still scaling up to large dataset sizes.  \n",
      "- **Iterative processing**  \n",
      "Many algorithms - such as those in machine learning - are iterative in nature, so it\u2019s much more efficient to hold each intermediate working set in memory, compared to loading from disk on each iteration. The architecture of MapReduce does not allow this, but it\u2019s straightforward with Spark, for example, and it enables a highly exploratory style of working with datasets.  \n",
      "- **Stream processing**  \n",
      "Streaming systems like Storm, Spark Streaming, or Samza make it possible to run real-time, distributed computations on unbounded streams of data and emit results to Hadoop storage or external systems.  \n",
      "    - **An aside: Lambda architecture** \n",
      "When you start talking about stream processing, inevitably comparisons to batch processing arise. A promising and buzzword-y topic recently has been so-called Lambda architecture, which seeks to combine the two. As architecture of production distributed systems is beyond the scope of the course, we'll merely point interested readers to [this piece](http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html) and the interesting discussion that follows in the comments (notably see Patrick Wendell's comment - he is a cofounder of Databricks and one of the core committers of Spark)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## MapReduce: \"a programming model for data processing\"\n",
      "- A style of computing developed by Google, whose version was written in C++. Hadoop's version is open-source and written in Java. \n",
      "- Each job consists of one mapper and one (optional) reducer. \n",
      "- Hadoop manages the parallel execution, coordination of tasks that execute Map or Reduce \n",
      "- Master node; worker (slave) nodes\n",
      "- Steps: input chunks, map tasks, group by keys, reduce tasks, output\n",
      "\n",
      "### Streaming (Python) vs. Java\n",
      "We won't be covering the Java language, and therefore Java MapReduce, in this class. However there are certain things Java MapReduce is better at doing than its streaming API; and certain operations such as joins are better to implement in Java than in streaming. The curious should refer to the [Java_Mapreduce_Examples](Java_Mapreduce_Examples.ipynb) notebook. \n",
      "\n",
      "**A design difference**   \n",
      "Hadoop Streaming uses the Unix standard streams as the interface between Hadoop and your program. Map input data is passed over stdin to your map function, which processes it line by line and writes to stdout. Input to the reduce function is stdin (which is guaranteed to be sorted by key) and writes results to stdout.  \n",
      "In Streaming, the map program can decide how to process the input: e.g. it can easily read and process multiple lines at a time. One could do that in Java, but to process multiple lines one would have to accumulate previous lines in an instance variable in the `Mapper`; then implement the `close()` method so you know when the last record has been read. \n",
      "\n",
      "**But mostly, you want to think about the tl;dr tradeoff**: human time vs. performance time.  \n",
      "- **Python** is easier and faster to develop, read, maintain. For the data scientist, assuming parallelizability, one can easily create production jobs from local prototypes. In addition you get the benefits of the rich scientific packages scikit-learn, numpy, scipy.  \n",
      "- **Java** performance is better. The reasons are beyond the scope of this course, but the curious should refer to these links about Python under the hood.  \n",
      "\n",
      "**The different types of Python**:  \n",
      "http://www.toptal.com/python/why-are-there-so-many-pythons  \n",
      "**The infamous global interpreter lock:**  \n",
      "http://en.wikipedia.org/wiki/Global_Interpreter_Lock\n",
      "https://wiki.python.org/moin/GlobalInterpreterLock"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Writing a MapReduce Job in Python\n",
      "\n",
      "### Word count, aka the \"Hello World\" of big data.\n",
      "\n",
      "Let's walk through an example of writing a mapper function, a reducer function, running it locally (on your Digital Ocean box) to test it, then running it using the Hadoop streaming jar \n",
      "\n",
      "In the `datacourse/scripts` dir you should see a `mapper.py` and a `reducer.py`.\n",
      "\n",
      "Let's make sure the code works as written.\n",
      "``` bash\n",
      "$ cat mkdirs4sbt | python mapper.py | sort -k1,1 | python reducer.py\n",
      "```\n",
      "This should have piped to stdout a word count on the bash script mkdirs4sbt. Notice that the output is ordered alphabetically on the first key. \n",
      "\n",
      "#### Running\n",
      "```bash\n",
      "export STREAMING_JAR=/usr/lib/hadoop-mapreduce/hadoop-streaming-2.2.0.2.0.11.0-1.jar\n",
      "\n",
      "$ hadoop jar $STREAMING_JAR \\\n",
      "-mapper datacourse/scripts/mapper.py \\\n",
      "-reducer datacourse/scripts/reducer.py \\\n",
      "-input gutenberg \\\n",
      "-output gutenberg_output\n",
      "```\n",
      "The mapper task converts the inputs into lines and feeds the lines into the stdin of the process. \n",
      "The output from stdout of the mapper process is converted into a key/value pair. By default, all characters up to the first tab character is the key and the rest of the line is the value. This is customizable.\n",
      "\n",
      "Optional streaming command parameters:  \n",
      "`-combiner, -inputformat, -outputformat, -numReduceTasks, -file` \n",
      "\n",
      "Optional generic command parameters:   \n",
      "`-D` Configuration variables, e.g.\n",
      "- ```-D mapred.reduce.tasks=0``` specifies a \"map-only\" job.\n",
      "- ```-D stream.map.output.field.separator=.``` specifies '.' as the field separator instead of '\\t'\n",
      "- `-D output.compression.enabled=true` \n",
      "- `-D output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec`\n",
      "\n",
      "For full documentation, see: https://hadoop.apache.org/docs/r1.2.1/streaming.html\n",
      "\n",
      "You can also provide a Java class to Streaming, like so:\n",
      "```\n",
      "$HADOOP_HOME/bin/hadoop jar $STREAMING_JAR \\\n",
      "    -input myInputDirs \\\n",
      "    -output myOutputDir \\\n",
      "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
      "    -reducer /bin/wc\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Mapreduce Steps\n",
      "![Mapreduce Process Structure from [Xiaocheng Zhang's blog](http://xiaochongzhang.me/blog/?p=338)](../images/MapReduce_Work_Structure.png)\n",
      "\n",
      "1. **Input**: the input is a large file on a distributed file system stored as an input system and read according to a user-speicifed `InputFormat`.  For word count, it is a large text file.\n",
      "1. **Splitting**: an Input Reader splits the file into chunks of 64 MB or 128 MB.  We think of each chunk as consisting of a collection of records which can be represented as key-value pairs `(k0, v0)`.  We think of the records as being unordered but keyed.  The splitting is done so that each chunk is also on a single machine.  While there can be multiple chunkcs on a single machine, it is useful to abstract things and assume that each chunk resides on its own machine.\n",
      "1. **Mapping**: the user specifies a `map` function, which iterates through the records within a chunk.  It outputs key-value pairs `(k1, v1)` (which are written to the local disk).  It can output multiple key-value pairs per input record or none at all and the key and value need not have the same type as the input.  In our case, each input record is a line in a file (e.g. `\"Deer Bear River\"`) and there is an output record consistoing of each word with the number one (e.g. `(\"Deer\", 1)`), indicating that we saw the word `\"Deer\"` once.\n",
      "1. **Combiner**: (skip this step the first time).  This runs a reduce right after mapping but before shuffling.  The **local aggregation** can reduce the amount of I/O in the shuffle phase, which is the most important bottleneck.\n",
      "1. **Shuffle**: up until this point, all the computations have happened locally and no data has been exchanged between the servers.  During Shuffle, the key value pairs `(k1, v1)` are sent to a computer based on `k1` so that all pairs with the same key end up at the same computer (and are written to that computer's disk).  Think of `k1` as giving the *address* to which to send the data and `v1` as givnig the data payload to deliver.  Again, which we can have multiple keys mapping to the same physical machine, it is useful to abstract this and assume each key residing on its own machine.\n",
      "1. **Reduce**: the user specifies a `reduce` function, which is given pairs `(k1, iter(v1))` where `iter(v1)` is an interator over the values.  An iterator is like a list except that there is no random access to elements and one can only iterate through its elements once in order.  Underneath the hood, all the data resides on disk and only the current record being iterated over is kept in memory.  The `reduce` function outputs key-value pairs `(k2, v2)` (which are written to the local disk).  Again, the number of values it outputs is independent of the number of inputs and they can be of different types.  In our case, it maps each `(k1, iter(v1))` to `(k1, sum(v1))`.\n",
      "1. **Final Result**: The results at this point are written out to disk according ot a user-specified `OutputFormat`.\n",
      "\n",
      "**Finer points**:\n",
      "1. The records in mapreduce are meant to be unordered.\n",
      "1. Mapreduce is fault tolerant, if one of the processes has an error, it will automatically be restarted.\n",
      "1. You might think it's rediculous to write `(\"Car\", 1)` twice for the input record `\"Car Car River\"`.  We could have written `(\"Car\", 2)`.  How much memory woud it require to keep track of this?  In general, we want the amount of memory used to be constant in the number of input records - this is what makes mapreduce scalable.  That said, you should look into the `combiner` function.  The typical use case is to run reduce on all the values `(k1, iter(v1))` which happen to be on the same mapper computer as a speed up.  To find out more, look at the [Apache tutorial](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html).  What does this save you, CPU time, RAM, or Network Load?\n",
      "1. For more information, checkout [Apache](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Job progression: \n",
      "1. Client applications submit jobs to the Job tracker.  \n",
      "2. The JobTracker talks to the NameNode to determine the location of the data\n",
      "3. The JobTracker locates TaskTracker nodes with available slots at or near the data\n",
      "4. The JobTracker submits the work to the chosen TaskTracker nodes.\n",
      "5. The TaskTracker nodes are monitored. If they do not submit heartbeat signals often enough, they are deemed to have failed and the work is scheduled on a different TaskTracker.\n",
      "6. A TaskTracker will notify the JobTracker when a task fails. The JobTracker decides what to do then: it may resubmit the job elsewhere, it may mark that specific record as something to avoid, and it may may even blacklist the TaskTracker as unreliable.\n",
      "7. When the work is completed, the JobTracker updates its status."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The Hadoop Ecosystem\n",
      "![A Hadoop stack](../images/Hadoop_ecosystem.png) \n",
      "\n",
      "## What makes Hadoop so great?\n",
      "- It's cheap: Hadoop itself is free; commodity clusters are inexpensive\n",
      "- vs. Relational Database Management Systems\n",
      ">  MapReduce is a good fit for problems that need to analyze the whole dataset in a batch fashion, particularly for ad hoc analysis. An RDBMS is good for point queries or updates, where the dataset has been indexed to deliver low-latency retrieval and update times of a relatively small amount of data. MapReduce suits applications where the data is written once and read many times, whereas a relational database is good for datasets that are continually updated.  \n",
      "Hadoop systems such as Hive are becoming more interactive (by moving away from MapReduce) and adding features like indexes and transactions that make them look more and more like traditional RDBMSs.\n",
      "\n",
      "- vs. Grid Computing\n",
      "> The high-performance computing (HPC) and grid computing communities have been doing large-scale data processing for years, using such application program interfaces (APIs) as the Message Passing Interface (MPI). Broadly, the approach in HPC is to distribute the work across a cluster of machines, which access a shared filesystem, hosted by a storage area network (SAN). This works well for predominantly compute-intensive jobs, but it becomes a problem when nodes need to access larger data volumes (hundreds of gigabytes, the point at which Hadoop really starts to shine), since the network band\u2010 width is the bottleneck and compute nodes become idle.\n",
      "Hadoop tries to co-locate the data with the compute nodes, so data access is fast because it is local. This feature, known as data locality, is at the heart of data processing in Hadoop and is the reason for its good performance. \n",
      "\n",
      "- Programming in Hadoop\n",
      "> MPI gives great control to programmers, but it requires that they explicitly handle the mechanics of the data flow, exposed via low-level C routines and constructs such as sockets, as well as the higher-level algorithms for the analyses. Processing in Hadoop operates only at the higher level: the programmer thinks in terms of the data model (such as key-value pairs for MapReduce), while the data flow remains implicit.\n",
      "\n",
      "### So, what's different about programming around big data?\n",
      "- Performance considerations: get rid of straggler tasks (e.g. resulting from [skew](http://nuage.cs.washington.edu/pubs/opencirrus2011.pdf)). Ex: When using MapReduce, optimize the number of reducers; add a `combiner`\n",
      "- Hardware configuring and tuning. \n",
      "- Monitoring the progress of your jobs\n",
      "- Often: opaque error messages.\n",
      "\n",
      "### The influencers in the Hadoop ecosystem...\n",
      "\n",
      "#### [Cloudera & Apache](http://vision.cloudera.com/the-ecosystem-has-no-center/#sthash.WHeLLLHi.dpuf)\n",
      "\n",
      "\u2022\tSchedulers like YARN & Mesos  \n",
      "\u2022\tCompute engines like MapReduce and Apache Spark  \n",
      "\u2022\tA multitude of SQL engines, including Apache Hive, Apache Drill, and Apache Phoenix  \n",
      "\u2022\tKeystores like Apache HBase and Apache Accumulo  \n",
      "\u2022\tMachine learning libraries like Apache Mahout and Spark\u2019s MLlib  \n",
      "\u2022\tStream-processing systems like Apache Storm, Apache Kafka, and Spark and so on.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### If Hadoop is not working on DO, you might need to restart\n",
      "```bash\n",
      "sudo su root  # become the root user\n",
      "echo JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64 >> /etc/environment\n",
      "echo JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64 >> /etc/alternatives/hadoop-conf/hadoop-env.sh\n",
      "\n",
      "# run command-D to get out of root mode\n",
      "\n",
      "# Start HDFS\n",
      "sudo service hadoop-hdfs-datanode start\n",
      "sudo service hadoop-hdfs-namenode init\n",
      "sudo service hadoop-hdfs-namenode start\n",
      "\n",
      "sudo -u hdfs hadoop fs -mkdir /user/\n",
      "sudo -u hdfs hadoop fs -mkdir /user/vagrant\n",
      "sudo -u hdfs hadoop fs -chown vagrant /user/vagrant\n",
      "\n",
      "# Check to make sure everything works now  \n",
      "hadoop fs -put datacourse/small_data/fha_by_tract.csv /user/vagrant\n",
      "```"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}