{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Python Mapreduce\n",
      "\n",
      "Yesterday we saw an example of using Hadoop Streaming's API in Python to implement a word count. Today we'll be taking a closer look at both Python MapReduce and how to run \"real\" jobs using Amazon Web Services. We'll also be taking a look at another implementation of word count in Python using a library called `mrjob`.\n",
      "\n",
      "### Why `mrjob`?\n",
      "`mrjob` is an open-source Python framework that provides a pythonic API to Hadoop Streaming and is actively developed by Yelp. Since Yelp operates entirely inside Amazon Web Services, mrjob\u2019s integration with EMR is very smooth and easy (using the boto package). We are partially using `mrjob` because it enables seamless resource-sharing on EMR, e.g. for the `mr.py` assignment.\n",
      "\n",
      "Some features:\n",
      "- Run jobs on EMR, your own Hadoop cluster, or locally (for testing).\n",
      "- Write multi-step jobs (one map-reduce step feeds into the next)\n",
      "\n",
      "However, keep in mind that `mrjob` *will* be slower than using the straight-up Streaming API. \n",
      "\n",
      "### Example\n",
      "This script [wordcount.py](../projects/mrjob/src/wordcount.py).\n",
      "\n",
      "The code consists mostly of a class that inherits from `MRJob` and has two components:\n",
      "\n",
      "1. A mapper which uses regular expressions to break up each line into words and yields examples of each\n",
      "```python\n",
      "def mapper(self, _, line):\n",
      "\tfor word in WORD_RE.findall(line):\n",
      "\t\tyield (word.lower(), 1)\n",
      "```\n",
      "2. A reducer that sums up all instances of the words\n",
      "```python\n",
      "def reducer(self, word, counts):\n",
      "\tyield (word, sum(counts))\n",
      "```\n",
      "**A note on generators:** functions that create iterators, for example with Python\u2019s yield statement. They have the advantage that an element of a sequence is not produced until you actually need it. This can help a lot in terms of computational expensiveness or memory consumption depending on the task at hand.\n",
      "\n",
      "**What's happening in this example:**\n",
      "A job is defined by a class that inherits from MRJob. This class contains methods that define the steps of your job.\n",
      "\n",
      "A \u201cstep\u201d consists of a mapper, a combiner, and a reducer. All of those are optional, though you must have at least one. So you could have a step that\u2019s just a mapper, or just a combiner and a reducer.\n",
      "\n",
      "When you only have one step, all you have to do is write methods called mapper(), combiner(), and reducer().\n",
      "\n",
      "The mapper() method takes a key and a value as args (in this case, the key is ignored and a single line of text input is the value) and yields as many key-value pairs as it likes. The reduce() method takes a key and an iterator of values and also yields as many key-value pairs as it likes. (In this case, it sums the values for each key, which represent the numbers of characters, words, and lines in the input.)\n",
      "\n",
      "Notice that mapper and reducer are not functions, they implement a many to many mapping.  `Map` takes a single line and outputs multiple words per line.  `Reduce` takes multiple instances of a word and outputs a single record per word.  In general, mapper and reducer implement many to many mappings.  In python, this is implemented via the `yield` keyword, which tells `mrjob` to write the answer to disk.  To invoke a mapreduce, run\n",
      "```bash\n",
      "python wordcount.py input_file.txt\n",
      "```\n",
      "to count the words in `input_file.txt`.\n",
      "\n",
      "**Exercise**:\n",
      "1. You might decide that this mapreduce is inefficient.  If the word `bear` appears twice in a line or node, it writes `(bear,1)` twice instead of `(bear,2)` once.  Since this item has to be sent across the network, this can be inefficient.  Fortunately, there is the notion of a `combiner`: for certain types of reducers, it [effectively runs reduce locally on the node](http://mrjob.readthedocs.org/en/latest/guides/concepts.html), potentially reducing network traffic.  [Here's some documentation on how to add a combiner to mrjob](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.combiner).  Add one to word count.\n",
      "1. Create a script called `src/unixwordcount.py`.  Modify this to output the number of characters, words, and lines in a string of text, i.e. to mimic the <a href=\"http://en.wikipedia.org/wiki/Wc_(Unix)\">unix word count program</a>.\n",
      "1. Use MR to comptue the total amount of sales per user in `data/sales.tsv`.\n",
      "1. Write a mapreduce to compute the mean and variance of `data/numbers.txt` without storing the entire sequence.  How would you add a combiner?\n",
      "1. Write a mapreduce to compute an approximate median of `data/numbers.txt`.  Hint, use [reservoir sampling](http://en.wikipedia.org/wiki/Reservoir_sampling).\n",
      "\n",
      "**Interview Question**: Let's say you're running a mapreduce job but at some step, a key is 'hot' and records with that key account for 90% of the data in that phase.  (For example, you are running total sales per country but the vast majority of your sales occur in the US.)  Since all the data with the same key gets sent to a single reducer, you have not taken advantage of the parralization in MR.  How would you solve this?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Joining Data\n",
      "\n",
      "Joining data in mapreduce which involves using multiple mappers for a mapreduce.  For example, let's say we want to to compute total sales by country.  We would have to join `data/sales.tsv` and `data/users.tsv` by user in one MR and aggregate the resulting table by country in the second MR.\n",
      "\n",
      "MR jobs are written to be composable: that is, we can take the output of one MR job and output it to another.  In `mrjob`, this is done by overriding the `steps` method to return a list of the individual `mr` classes which call mappers and reducers defined above in the class.  Check out the [mrjob documentation](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#multi-step-jobs) for more details.\n",
      "\n",
      "**Exercise**: Join `data/sales.tsv` and `data/users.tsv` to compute totla sales by country.  *Hint*: `mrjob` can take any number of arguments and all their contents get fed into mapper.  In the mapper, you will need to use the values in the first column to determine which table you are reading."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multistep Mapreduce\n",
      "\n",
      "Another classic example of an MRJob that requires two steps is squaring a matrix $M$.  The two steps are:\n",
      "1. The first mapreduce emits $M_{ij}$ keyed by both $i$s and $j$'s (map) and then performing the multiplication $M_{ij} * M_{jk}$ (reduce) keyed by $(i,k)$.\n",
      "2. The second mapreduce sums the values of $M_{ij} * M_{jk}$ along $j$ (reduce: it has an identity mapper).\n",
      "\n",
      "The algorithm is given in `src/matrix_square.py`.\n",
      "\n",
      "**Exercises**: write a new `src/matrix_multiply.py` that multiplies the matrices stored in `A.csv` and `B.csv`.\n",
      "\n",
      "**Interview Question**: How much memory does normal matrix multiplication take?  How much memory does MR matrix multiplication take?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercises**:\n",
      "\n",
      "1.  Write a mapreduce to get total purchases per user from [`transactions.tsv`](../projects/MR/data/transactions.tsv).  For more information, take a look at [Hadoop and Java_Mapreduce](Hadoop_and_Java_Mapreduce.ipynb).\n",
      "\n",
      "1.  Now join in [`users.tsv`](../projects/MR/data/users.tsv) by userid to get the amount spent in each country.\n",
      "\n",
      "1.  Write a mapreduce to multiply two matrices $A_{ij}$ and $B_{ij}$.  Assume that the elements of $A_{ij}$ are encoded sparsely in a `csv` format:\n",
      "\n",
      "        \"A\", i, j, v\n",
      "    \n",
      "    That is, `i` and `j` are the indices and `v` is the value at position `i` and `j`.  $B_{ij}$ is similar.  Note that the first column is there to distinguish between the two matrices becasue `mrjob` only takes one input file.  Hint, this requires two mapreduces chained together.\n",
      "1.  Compute the set of all common friends between two individuals via a mapreudce.  Assume that the friendship matrix is encoded sparsely in a `csv` format:\n",
      "\n",
      "        u1, u2\n",
      "        \n",
      "    The above row implies, `u1` is friends with `u2` (and this is a symmetric relationship).  The output should be of the form\n",
      "    \n",
      "        u1, u2, 24\n",
      "    \n",
      "    which would represent that there are 24 common friends shared by `u1` and `u2`.  Can you reuse your solution to the previous mapreduces to help?\n",
      "1.  Given a stream of data $X_1, X_2, \\ldots$, compute its mean, standard deviation.  Also comupte its median, and 25%, 75% quantiles.  *Hint:* you need ot use [reseroir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling).\n",
      "1.  How would we normalize features before submitting them to a linear regression?  That is, given features $X_{ij}$ in the above format, we want a mapreduce which outputs $X'_{ij}$ where\n",
      "\n",
      "    $$ X'_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j} $$\n",
      "    \n",
      "    where $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation of column $j$ of $X_{ij}$.  *Hint:* you need to compute the mean and standard deviation in the mapper and send them first to the reducer before you send the datapoints $X_{ij}$.  *Hint:* you may need use secondary sort.\n",
      "1.  Write a mapreduce implementation of Naive Bayes.  The solution is what is called an *online* as opposed to *offline* algorihtm.  Assume the input data are rows of a feature matrix $X_{ij}$ given in a `csv` format\n",
      "\n",
      "        f1, f2, f3, ..., fp\n",
      "\n",
      "    where `f1`, ... `fp` are floats that represent the $p$ features of a row in $X$.  The output should be one row\n",
      "    \n",
      "        b1, b2, b3, ..., bp\n",
      "    \n",
      "    where `b1`, ..., `bp` are the components of $\\beta$, the coefficients of the model.  When would this be useful?\n",
      "1.  Write linear or logistic regression as a mapreduce with the same input and output.\n",
      "1.  What other machine-learning algorithsm can you write as a mapreduce?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2014 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}