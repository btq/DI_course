{
 "metadata": {
  "name": "",
  "signature": "sha256:8a2a8217fc3896d6c56d5e1595c182cf45da8e8fd370ae21f26e0ae2d5ba9ca7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Scalding! (*Scala on Cascading*)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Refer to https://github.com/twitter/scalding for the latest and greatest in Scalding documentation.\n",
      "\n",
      "### Contents\n",
      "1. [Background](#background)\n",
      "1. [The Scalding API](#api)\n",
      "1. [The canonical example](#countingwords)\n",
      "1. [Tools set-up](#setup)\n",
      "1. [A big data exercise](#exercise)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='background'></a>\n",
      "### Background: Frameworks on top of MapReduce\n",
      "By now you've been exposed to two frameworks of distributed computing: Hadoop MapReduce and Apache Spark. Spark showed us how \"nice\" programming around big data can be in terms of expressing your algorithms in terms of primitives and functions that allow you to think more abstractly than \"key, value, shuffle, map-and-reduce-only.\" However there are still many situations (e.g. [while we wait for Spark-on-YARN to truly scale nicely](http://blog.explainmydata.com/2014/05/spark-should-be-better-than-mapreduce.html)) in which we would still like our cluster manager to run MapReduce under the hood without sacrificing convenient utilities like joins.\n",
      "\n",
      "Thankfully, as we alluded to in [Intro to Distributed Computing](./Intro_Distributed_Computing.ipynb), several APIs exist on top of MapReduce that allow exactly this: [Pig](https://pig.apache.org/docs/r0.7.0/piglatin_ref2.html), [Crunch](https://crunch.apache.org), and [Cascading](http://www.cascading.org) are the most popular; they each offer benefits and tradeoffs over one another. \n",
      "\n",
      "We'll introduce Scalding, the Scala DSL of Cascading, primarily because:  \n",
      "1. we've already been introduced to Scala via Spark,  \n",
      "1. Scalding has wide usage in sophisticated production settings among a number of recognizable companies,  \n",
      "1. it has an advantage over the UDFs and SQL-like operations of Pig because Pig requires one to maintain two codebases for it to be used in production (one for Pig operations, the other in Python/Java to call the Pig operations)\n",
      "1. for a comparison to Crunch, refer [here](#crunch_difference)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='api'></a>\n",
      "\n",
      "### Scalding API Primer\n",
      "\n",
      "Scalding's **functions**, i.e. the \"verbs\" of the DSL, can be divided into four types:\n",
      "- Map-like functions\n",
      "- Grouping functions\n",
      "- Group/Reduce functions\n",
      "- Join operations\n",
      "And other misc. functions\n",
      "\n",
      "Scalding's key **objects**, i.e. the \"nouns\":\n",
      "- TypedPipe[T] : distributed list of objects of type T\n",
      "- KeyedList[K,V] : represents some sharding of objects of key K and value V\n",
      "    - Grouped[K,V] : usual groupings\n",
      "    - CoGrouped[K,V] : cogroupings / joins\n",
      "\n",
      "**APIs**  \n",
      "- Fields-based API (legacy)\n",
      "- Type-safe API (use this!)\n",
      "- Matrix API "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='countingwords'></a>\n",
      "\n",
      "## Counting words in Scalding  \n",
      "\n",
      "#### The \"Scalding\" way to approach the word count algorithm:\n",
      "- Have a distributed list of objects of type `String`, i.e. a `TypedPipe[String]` resulting from reading in a text file  \n",
      "- From the `TypedPipe`, generate a flattened list of words. Split tokens on one or more whitespace characters, i.e. with regex: `\\\\s+`  \n",
      "- Group this list of words by word  \n",
      "- Count the number of occurences of each word\n",
      "- Write tab-separated word (String), count (Long) to file  \n",
      "\n",
      "#### Script that does the above\n",
      "```scala\n",
      "import com.twitter.scalding._\n",
      "\n",
      "val lines : TypedPipe[String] = TypedPipe.from(TextLine(\"hello.txt\"))\n",
      "\n",
      "// Write a word count to file\n",
      "lines.flatMap(_.split(\"\\\\s+\"))\n",
      "    .group\n",
      "    .sum\n",
      "    .write(TypedTsv[(String, Long)](\"output\"))\n",
      "```    \n",
      "    \n",
      "#### Scala class (that is also stylistically nicer) that does the above:\n",
      "```scala\n",
      "import com.twitter.scalding._\n",
      "\n",
      "class WordCountJob(args: Args) extends Job(args) {\n",
      "  TypedPipe.from(TextLine(args(\"input\")))\n",
      "    .flatMap { line => tokenize(line) }\n",
      "    .groupBy { word => word } // use each word for a key\n",
      "    .size // in each group, get the size\n",
      "    .write(TypedTsv[(String, Long)](args(\"output\")))\n",
      "\n",
      "  // Split a piece of text into individual words.\n",
      "  def tokenize(text : String) : Array[String] = {\n",
      "    // Lowercase each word and remove punctuation.\n",
      "    text.toLowerCase.replaceAll(\"[^a-zA-Z0-9\\\\s]\", \"\").split(\"\\\\s+\")\n",
      "  }\n",
      "}\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='setup'></a>\n",
      "## Getting set up on Scalding \n",
      "\n",
      "<pre><code>$ git clone https://github.com/twitter/scalding.git\n",
      "$ ./sbt update\n",
      "$ ./sbt test     # runs the tests; if you do 'sbt assembly' below, these tests, which are long, are repeated\n",
      "$ ./sbt assembly # creates a fat jar with all dependencies, which is useful when using the scald.rb script\n",
      "</code></pre>\n",
      "Also add a SCALDING_HOME variable to your ~/.bash_profile pointing to the Scalding directory. \n",
      "\n",
      "#### Scalding REPL\n",
      "To open the REPL:\n",
      "`$ $SCALDING_HOME/scripts/scald.rb --repl --local`\n",
      "\n",
      "Walk through this REPL example at https://gist.github.com/johnynek/a47699caa62f4f38a3e2 to get a feel for the Scalding programming paradigms."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Running your Scalding code locally\n",
      "\n",
      "`$ scald.rb --local {YourApp.scala}`\n",
      "\n",
      "Your build.sbt should look something like this:  \n",
      "```scala\n",
      "name := {Your App Name}\n",
      "version := \"1.0\"\n",
      "scalaVersion := \"2.10.4\"\n",
      "\n",
      "resolvers += \"Concurrent Maven Repo\" at \"http://conjars.org/repo\"\n",
      "\n",
      "libraryDependencies += Seq(\n",
      "    \"cascading\" % \"cascading-core\" % \"2.0.2\",\n",
      "    \"cascading\" % \"cascading-local\" % \"2.0.2\",\n",
      "    \"cascading\" % \"cascading-hadoop\" % \"2.0.2\",\n",
      "    \"cascading.kryo\" % \"cascading.kryo\" % \"0.4.4\",\n",
      "    \"com.twitter\" % \"meat-locker\" % \"0.3.0\",\n",
      "    \"com.twitter\" % \"maple\" % \"0.2.2\",\n",
      "    \"commons-lang\" % \"commons-lang\" % \"2.4\",\n",
      "    \"com.twitter\" % \"scalding_2.9.2\" % \"0.7.3\",\n",
      "    \"org.specs2\" % \"specs2_2.9.2\" % \"1.12.1\"\n",
      "    )\n",
      "```\n",
      "\n",
      "### Running on Amazon EMR \n",
      "\n",
      "Because Scalding is Hadoop MapReduce under the hood, unlike in the case of Spark where we had to follow special submission instructions, submitting a `jar` of Scalding code works in the same way as submitting a `jar` of straight-up Java MapReduce code. To build your project, run `sbt assembly` in the root directory of your project.\n",
      "\n",
      "You'll fire up an EMR cluster as per usual (use `./scripts/launch_emr.sh` as a guide) and submit a step either during the creation or after the cluster is running through the EMR Console or the AWS CLI.\n",
      "\n",
      "Official EMR documentation is here: http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CreateCascading.html\n",
      "\n",
      "TL;DR the command to create a cluster and add a step should be something like:\n",
      "```bash\n",
      "aws emr create-cluster --name \"Scalding job cluster\" --ami-version 3.6 \\\n",
      "--use-default-roles --ec2-attributes KeyName=myKey \\\n",
      "--instance-type m3.xlarge --instance-count 3 \\\n",
      "--bootstrap-actions Path=pathtobootstrapscript,Name=\"CascadingSDK\" \\\n",
      "--steps Type=\"CUSTOM_JAR\",Name=\"Scalding Step\",ActionOnFailure=CONTINUE,Jar=s3://pathtojarfile,\\\n",
      "Args=[\"-input\",\"s3://pathtoinputdata\",\"-output\",\"s3://pathtooutputbucket\",\"arg1\",\"arg2\"]\n",
      "```\n",
      "\n",
      "To add to a running cluster:\n",
      "```bash\n",
      "aws emr add-steps --cluster-id j-XXXXX --steps Type=\"CUSTOM_JAR\",Name=\"Scalding Step\",ActionOnFailure=CONTINUE,Jar=s3://pathtojarfile,\\\n",
      "Args=[\"-input\",\"s3://pathtoinputdata\",\"-output\",\"s3://pathtooutputbucket\",\"arg1\",\"arg2\"]\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='exercise'></a>\n",
      "## Exercise: Analyzing Wikipedia Traffic in 2008\n",
      "\n",
      "To tackle this exercise, you can use your choice of big-/small-data tools since the data is on `s3` and you can therefore fire up an EC2/EMR cluster installed with the tools of your choice. We suggest using either Spark or Scalding. In both cases, you should be able to complete all the questions using just that one tool (i.e. both the questions that require big-data-munging and those that can be answered in-memory); however you may also choose to use a mix of Python Pandas for the in-memmory-size data (e.g. for small time series calculations) after ETL'ing with one of the big data frameworks. \n",
      "\n",
      "If you answer all the questions in the exercise: congratulations! You'll have completed a full data scientist workflow using \"production-like\" data. You'll get a feel for how the workflow for big data differs from small data. You'll also gain an interesting talking point to potential employers.\n",
      "\n",
      "### About the dataset\n",
      "\n",
      "The English language dataset is located at `s3://thedataincubator-course/wikidata/wikistats/en_pagecounts` and derived from https://aws.amazon.com/datasets/Encyclopedic/2596. It's about 20 GB compressed and covers the period 10/1/08 through 12/8/08.\n",
      "\n",
      "(If you want to look at the entire dataset i.e. not just English-language, the full wikistats pagecount dataset is at `s3://thedataincubator-course/wikidata/wikistats/pp_pagecounts/` is ~75 GB and covers the period 10/1/08 through 12/8/08.)\n",
      " \n",
      "Each log has 4 fields: `yearmonthdayhour, projectcode, pagename, pageviews`\n",
      "```\n",
      "2008120701 en Still_Breathing 2\n",
      "2008120701 en Still_Climbing_%28album%29 1\n",
      "2008120701 en Still_Creepin_On_Ah_Come_Up 1\n",
      "2008120701 en Still_Life_with_Spherical_Mirror 2\n",
      "2008120701 en Stillwater_Mining_Company 2\n",
      "```\n",
      "\n",
      "### Questions (for the English language dataset only)\n",
      "1. Give 100 most popular websites by pageviews from 10/1/08-10/15/08 (inclusive), ordered by pageviews descending.\n",
      "\n",
      "1. For (start date) to (end date): Visualize the distribution of pages' cumulative pageviews during this time period. Give the mean, median, stdev. What kind of distribution is it?\n",
      "\n",
      "1. For the topic \"Barack Obama\": Look at the time series of daily page views from (start) to (end) date. Calculate the correlation with topic \"Sarah Palin.\"\n",
      "\n",
      "1. Bonus: How does the PageRank of a page predict its mean views over time?\n",
      "\n",
      "1. For a sample of pairs of linked nodes, calculate the pageview correlations. How does the the distribution of correlations compare to that of a randomly chosen sample of unlinked node pairs?\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='crunch_difference'></a>\n",
      "**What is the main difference between Crunch and Cascading?**  \n",
      "(Source: https://github.com/cloudera/crunch/wiki/Frequently-Asked-Questions)  \n",
      "\n",
      "The main difference between Crunch and Cascading/Pig/Hive is in their data models. In Pig and Cascading, most operations in a pipeline are performed on collections of Tuples (here are the Javadocs for the Pig Tuple and the Cascading Tuple). In my answer to this question on Quora, I refer to this as the \"single serializable type\" (SST) model. Using the SST data model makes it much easier to implement common operations, and Pig, and Cascading provide big libraries of built-in functions that are designed to operate on their respective Tuple types. They also provide APIs for developers to create their own user-defined functions that interact with their SST data models.\n",
      "\n",
      "Crunch, like the FlumeJava library that is based on, uses a data model that has multiple serializable types (MST). At each stage in a Crunch pipeline, you specify how the data from that stage should be serialized. The benefit of doing this is that it lets you verify at compile-time that each stage of your pipeline will actually receive the type of data it knows how to process. In this sense, the MST model for building pipelines is similar to using a statically typed language, and the SST model is similar to using a dynamically typed language. We feel that the MST model has definite benefits for MapReduce developers:\n",
      "\n",
      "Compile-time type verification lowers the probability that a type error will cause your MapReduce pipeline to fail when it actually runs on a Hadoop cluster.\n",
      "It makes user-defined functions in Crunch extremely easy to write, saving you from writing the boilerplate for type checking your data that you need to do in Pig or Cascading.\n",
      "It makes MapReduces that run over complex data types, like binary data or the results of an HBase scan, much easier to write. There's no work required to map from the complex type onto the Cascading/Pig SST and back again.\n",
      "Crunch's MST serialization model currently has two different implementations, one based on Writables and the other based on Avro records."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}