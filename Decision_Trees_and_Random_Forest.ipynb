{
 "metadata": {
  "name": "",
  "signature": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Decision Trees and Random Forests\n",
      "*&copy; The Data Incubator*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Decision Trees\n",
      "\n",
      "A decision tree is a binary tree.  At each of the internal nodes, it chooses a feature $i$ and a threshold $t$.  Each leaf has a value.  Evaluation of the model is just traversal of the tree from the root.  At each node, for example $j$, we go down the left branch if $X_{ji} \\le t$ and the right branch otherwise.  The value of the model $f(X_{ji})$ is the value at the value at the terminating leaf of this traveral.  Below, we show a picture of this on small decision tree trained on the iris data set.  Notice that each internal node has a decision criterion and each leaf has the breakdown of label classes left at this leaf of the tree.  For a geometric picture of a decision tree, take a look at this [blog post](https://shapeofdata.wordpress.com/2013/07/02/decision-trees/)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train and draw out a decision tree\n",
      "\n",
      "from IPython import display\n",
      "from sklearn import datasets, tree, utils\n",
      "from sklearn.externals.six import StringIO  \n",
      "import pydot \n",
      "\n",
      "# Train a small decision tree on the iris dataset\n",
      "dataset = datasets.load_iris()\n",
      "X_iris, y_iris = utils.shuffle(dataset.data, dataset.target, random_state=42)\n",
      "tree_clf = tree.DecisionTreeClassifier(max_depth=3).fit(X_iris, y_iris)\n",
      "\n",
      "# Generate a plot of the decision tree\n",
      "dot_data = StringIO() \n",
      "tree.export_graphviz(tree_clf, out_file=dot_data) \n",
      "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
      "display.Image(graph.create_png())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Decision Tree Training Algorithm and Tuning Parameters\n",
      "\n",
      "The algorithm to construct a Decision Tree recursively builds a tree structure.  At each node, it finds the split (the feature and threshold level) that maximize the improvement in a criteria (in this case, the decrease in the gini index.  This algorithm is controlled by four major parameters:\n",
      "\n",
      "<table>\n",
      "\t<tr>\n",
      "    <th>Feature</th>\n",
      "    <th>Value</th>\n",
      "\t</tr>\n",
      "\n",
      "\t<tr>\n",
      "    <td>`max_features`</td>\n",
      "    <td>The number of features to consider when choosing a split for an internal node</td>\n",
      "\t</tr>\n",
      "\n",
      "\t<tr>\n",
      "    <td>`max_depth`</td>\n",
      "    <td>The maximum depth of tree from the root</td>\n",
      "\t</tr>\n",
      "\n",
      "\t<tr>\n",
      "    <td>`min_samples_split`</td>\n",
      "    <td>Minimum number of samples required for a split to be considered</td>\n",
      "\t</tr>\n",
      "\n",
      "\t<tr>\n",
      "    <td>`min_samples_leaf`</td>\n",
      "    <td>Minimum number of samples required for each leaf</td>\n",
      "\t</tr>\n",
      "</table>\n",
      "\n",
      "**Question**: \n",
      "\n",
      "1. How do each of these features affect the Variance Bias Decomposition?\n",
      "1. The following snippet trains a very basic Decision Tree on the Boston housing price dataset with default parameters.  Adjust the above parameters to tune a better decision tree.  How much better is it's performance?  Hint: use `sklearn.grid_search.GridSearchCV`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train a basic decision tree.  Can you do better?\n",
      "from sklearn import cross_validation\n",
      "import pandas as pd\n",
      "\n",
      "boston = datasets.load_boston()\n",
      "\n",
      "columns = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSAT\"]\n",
      "X = pd.DataFrame(boston.data, columns=columns)\n",
      "y = pd.Series(boston.target)\n",
      "\n",
      "cv = cross_validation.ShuffleSplit(len(y), n_iter=20, test_size=0.2, random_state=42)\n",
      "def compute_error(clf, X, y):\n",
      "    return - cross_validation.cross_val_score(clf, X, y, cv=cv, scoring='mean_squared_error').mean()\n",
      "\n",
      "tree_reg = tree.DecisionTreeRegressor()\n",
      "pd.DataFrame([\n",
      "    (\"Mean Model\", y.var()),\n",
      "    (\"Decision Tree\", compute_error(tree_clf, X, y))\n",
      "], columns=[\"Model\", \"MSE\"]).plot(x=\"Model\", y=\"MSE\", kind=\"Bar\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Random Forests\n",
      "\n",
      "A random forest is just an ensemble of decision trees.  The predicted value is just the average of the trees (for both regression and classification problems - for classification problems, it is the probabilities that are averaged).  You can adjust `n_estimators` to change the number of trees in the forest.  If each tree is trained on the same subset of data, why aren't they identical?  Two reasons:\n",
      "1. **Subsampling**: each tree is actually trained on a random selected (with replacement) subset (i.e. bootstrap)\n",
      "1. **Maximum Features**: the optimal split comes from a randomly selected subset of the features.  In scikit-learn, this feature is controlled by `max_features`.\n",
      "\n",
      "\n",
      "## Extremely Random Forests\n",
      "Instead of choosing the optimal split amongst a (randomly selected) subset of features, we choose random values we choose amongst randomly generated thresholds.  While the first two are options in scikit, this is implemented in `ExtraTreesClassifier`.\n",
      "\n",
      "**Question**: What happens to bias and variance of the individual trees in the averaging process of Random Forests and Extremely Random Forests.  How would you change your parameters to compensate?\n",
      "\n",
      "You can read more about these [here](http://scikit-learn.org/0.12/modules/ensemble.html).\n",
      "\n",
      "## Random Forest Training Algorithm and Tuning Parameters\n",
      "\n",
      "A Random Forest and Extremely Random Forest are pretty straightforward to train once you know how a Decision Tree works.  In fact, their construction can even be parallelized.  They have an extra parameter `n_estimators` and their construction can be parallelized by setting the parameter `n_jobs`.\n",
      "\n",
      "**Exercise**: the following code trains a random forest and an extra random forest with default parameters.  Can you do better?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train random forests with default parameters.  Can you do better?\n",
      "from sklearn import cross_validation, ensemble\n",
      "import pandas as pd\n",
      "\n",
      "tree_reg = tree.DecisionTreeRegressor()\n",
      "extra_reg = ensemble.ExtraTreesRegressor()\n",
      "forest_reg = ensemble.RandomForestRegressor()\n",
      "\n",
      "model_performance = pd.DataFrame([\n",
      "    (\"Mean Model\", y.var()),\n",
      "    (\"Decision Tree\", compute_error(tree_reg, X, y)),\n",
      "    (\"Random Forest\", compute_error(forest_reg, X, y)),\n",
      "    (\"Extra Random Forest\", compute_error(extra_reg, X, y)),\n",
      "], columns=[\"Model\", \"MSE\"])\n",
      "model_performance.plot(x=\"Model\", y=\"MSE\", kind=\"Bar\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gradient Boosting Trees\n",
      "\n",
      "### Summary:\n",
      "Gradient Boosting algorithms train on the residual error.  It does this by iteratively training a sequence of models $f_m$ which are based on the residual error from the previous model.  Each one is reducing the residual error left from the previous prediction.  If we have a model prediction $f_{m-1}(X_{j \\cdot})$ meant to predictvalue $y_j$, then the next model $f_m$ should train on\n",
      "\n",
      "$$ y'_j = y_j - f_{m-1}(X_{j \\cdot})\\,.$$\n",
      "\n",
      "### Details:\n",
      "The basic model can be written as\n",
      "\n",
      "$$ f(X) = f_m(X) = \\sum_{k=1}^m \\gamma_m h_m(x) $$\n",
      "\n",
      "where $h_m$ are called **weak learners**.  For Gradient Boosting Trees, the $h_m$ are a collection of stump trees (very short trees, often of height 1).  At each iteration, we estimate everything in two steps:\n",
      "\n",
      "$$ h_m = \\mbox{argmin}_{\\gamma, h} \\sum_{i=1}^n \\left[ - \\partial_f L(y_i, f_{m-1}(X_{i \\cdot})) - \\gamma h(X_{i \\cdot}) \\right]^2 $$\n",
      "\n",
      "where $L(y, f(X))$ is the loss function we are trying to minimize.  In words, we compute the negative gradient $- \\partial_f L(y_i, f_{m-1}(X_{i \\cdot}))$ and then find the values of $\\gamma$ and $h$ such that $\\gamma h(X_{i \\cdot})$ best approximates it (approximate in the sense of $L^2$ norm).  We throw away $\\gamma$ and keep $h$ as $h_m$.  Remember that there are a finite collection of $h_m$ and once $h_m$ is set, the solution to $\\gamma$ is closed-form becasue of the quadratic form of the potential.\n",
      "\n",
      "We then set $\\gamma_m$ by a one-dimensional linear search of the true loss function with our new $h_m$\n",
      "\n",
      "$$ \\gamma_m = \\mbox{argmin}_{\\gamma} \\sum_{i=1}^n L(y_i, f_{m-1}(X_{i \\cdot} + \\gamma h_m(X_{i \\cdot})) $$\n",
      "\n",
      "We can then set\n",
      "\n",
      "$$ f_m(x) = f_{m-1}(x) + \\gamma_m h_m(x) $$\n",
      "\n",
      "as our new predictor and iterate on the residuals.  There are a few basic variants that are usually applied\n",
      "\n",
      "1.  **Learning Rate**: we usually reduce the learning rate $\\alpha$ by choosing a value $0 < \\alpha < 1$ such that\n",
      "\n",
      "    $$ f_m(x) = f_{m-1}(x) + \\alpha \\gamma_m h_m(x) $$\n",
      "\n",
      "    Setting $\\alpha = 1$ would yield the model present above.  This slows down the rate at which we learn.\n",
      "1.  **Subsampling**: we can learn on a fraction of the data which is a subsample of the full dataset.\n",
      "1.  **Max Features**: we can learn from only a subset of the features (i.e. trees only select a subset of the features).\n",
      "1.  **Trying out different loss fucntions**: We can choose different functional forms $L$.\n",
      "\n",
      "You can learn more about gradient boosting [here](http://scikit-learn.org/0.12/modules/ensemble.html).\n",
      "\n",
      "**Question**: \n",
      "\n",
      "1. How do these techniques affect the bias and variance of the learned models?\n",
      "2. The following code trains a random forest and an extra random forest with default parameters.  Can you do better?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# GradientBoostingRegressor\n",
      "\n",
      "from sklearn import metrics\n",
      "\n",
      "###############################################################################\n",
      "# Fit regression model\n",
      "params = {'n_estimators': 1000, 'max_depth': 4, 'min_samples_split': 1,\n",
      "          'learning_rate': 0.02, 'loss': 'ls'}\n",
      "\n",
      "gradient_reg = ensemble.GradientBoostingRegressor(**params)\n",
      "\n",
      "gradient_performance = pd.DataFrame([\n",
      "    (\"Gradient Boosted Regressor\", compute_error(gradient_reg, X, y))\n",
      "], columns=[\"Model\", \"MSE\"])\n",
      "model_performance.append(gradient_performance).plot(x=\"Model\", y=\"MSE\", kind=\"Bar\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Importance\n",
      "\n",
      "With a linear regression, we could inspect to coefficients to tell which terms were the most important.  With decision trees and random forests, the model structure is much more opaque.  Fortunately, all of these models (once trained) have an array called `feature_importances_` which gives the imporances of all the features.  How do they compute feature importance?  According to the [scikit documentation](http://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation):\n",
      "\n",
      "> The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree are used contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features.\n",
      "\n",
      "Let's show this in action.  For scikit, larger values are more important and the values of `feature_importances_` sum to 1:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame([\n",
      "    tree_reg.fit(X, y).feature_importances_,\n",
      "    forest_reg.fit(X, y).feature_importances_,\n",
      "    extra_reg.fit(X, y).feature_importances_,\n",
      "    gradient_reg.fit(X, y).feature_importances_\n",
      "], columns=columns, index=[\"Tree\", \"Forest\", \"Extra Random\", \"Gradient\"]).T.plot(kind=\"bar\").legend(loc=\"upper left\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Questions**:\n",
      "What is the tradeoff between Decision Trees, Random Forests, and Gradient Boosting Trees, in terms of explicability, computational time, memory, and accuracy?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Spoilers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Answers\n",
      "\n",
      "### Decision Trees\n",
      "1. Increasing `max_features` and `max_depth` and decreasing `min_samples_split` and `min_samples_leaf` tend to build more complex models (increase Variance and reduce Bias).\n",
      "1. Straightfoward.\n",
      "\n",
      "### Random Forests\n",
      "The variance between different trees tends to cancel each other while the biases reinforce each other.  That is, becasue the trees are different, they tend to overfit in different ways but when they underfit, they underfit the same way.  So you want to use higher variance, lower bias parameters than you would with a decision tree.\n",
      "\n",
      "### Gradient Boosting Trees\n",
      "Increasing the learning rate increases variance.  Decreasing the fraction subsampled and the max features will decrease the variance.le\n",
      "\n",
      "### Final Question\n",
      "Decision Trees are clearly the least accurate although possibly the most explicable and they train quickly.  (Extra) Random Forests are more expensive to train but can be trained in parallel -- both across processors on the same data in one machine and across machines by splitting up the sample data.  Gradient Boosting Trees are not as naturally parallelizable and (empirically) seem to be the slowest."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Copyright &copy; 2014 The Data Incubator.  All rights reserved.*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}